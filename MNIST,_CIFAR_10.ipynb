{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCwJdcgnYq_e"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WTKscfc5__8r",
    "outputId": "409269c2-b35c-42f8-af98-e91124960ca3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Activation, AveragePooling2D, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aSKbU-aLYuuE"
   },
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "VNyuntEWBjby",
    "outputId": "626b55e3-db14-46e4-9ce5-9a05d1780a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxiGnNNPYz3a"
   },
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "pTeE4VNWADDA",
    "outputId": "d320b953-3144-4c38-d9e6-250bdd49eb82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(784,))\n",
    "encoded = Dense(32, activation='relu')(input)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZV6O7oFHAyzT"
   },
   "outputs": [],
   "source": [
    "autoencoder = Model(input, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zWvgQoWA_HG"
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=sgd, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2eSpKqfFZArg"
   },
   "source": [
    "## Prepare data and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kif9SEuqBfWF"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WuN2fWNiBG9_"
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.0016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "Z1VHXlTHBp1l",
    "outputId": "a175e95f-e7f3-40d5-f06a-04ed220c96fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0990 - val_loss: 0.0977\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0990 - val_loss: 0.0977\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=10,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Rxo51N5ZFKt"
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUngOheWHbqW"
   },
   "outputs": [],
   "source": [
    "preds = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "AgECoqTSBtbJ",
    "outputId": "61963e70-ed38-4e80-b5c5-0bf1b3814a3f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAADjCAYAAADdR/IFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAAAAOfH/LSLkkBsgMuEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(preds[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_X6JmycxK3Kj"
   },
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXuybEKkMvIq"
   },
   "source": [
    "Code from [this file](https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-6RhriFZJmV"
   },
   "source": [
    "## Set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0913y7y2K2pC"
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "\n",
    "# Subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuI8VjbIQd7p"
   },
   "outputs": [],
   "source": [
    "n = 3  # for ResNet20 v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RII1qp_-Mm4D"
   },
   "outputs": [],
   "source": [
    "version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ia3cTN4LMpY1"
   },
   "outputs": [],
   "source": [
    "# Computed depth from supplied model parameter n\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAha6NB3ZNfp"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "6Qk4pux8Mt22",
    "outputId": "52e43706-e85d-4103-ebdf-cee8bcc35ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 46s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAtreAxuM4HO"
   },
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1nmJ-GsM7ap"
   },
   "outputs": [],
   "source": [
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gUinyxzkM9m-"
   },
   "outputs": [],
   "source": [
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "nDfZ-zqMNBca",
    "outputId": "c8a2c3aa-c250-4b99-e082-05a2bbc1a620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYOHxoTHNEK1"
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_9_wczRZTTE"
   },
   "source": [
    "## Define ResNet blocks and architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZwS_zoZINGwk"
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4O29WPiaNcnF"
   },
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUfWaLkCNufI"
   },
   "outputs": [],
   "source": [
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCSnttUgZZK1"
   },
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvhlyL9UN4K8"
   },
   "outputs": [],
   "source": [
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAE7vGFWOX1Q"
   },
   "source": [
    "## Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "-OHCAUVWOcfL",
    "outputId": "d29c3341-8853-49ba-be67-e8d29af6b59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "root_dir = \"/content/gdrive/My Drive/\"\n",
    "base_dir = root_dir + 'fastai-v3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vf04ZNJnN72R"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(1e-3, decay=1e-3),  # we will compute this later\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97PYl3YtO-sF"
   },
   "outputs": [],
   "source": [
    "save_dir = 'gdrive/My Drive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_W93o8WoPMB2"
   },
   "outputs": [],
   "source": [
    "model_name = 'cifar10_model.{epoch:03d}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQRHKcUHPj3z"
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(save_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLqsZA2JPR9Y"
   },
   "outputs": [],
   "source": [
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEWnP96ZZkWJ"
   },
   "source": [
    "## Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTc4S4RNPgUt"
   },
   "outputs": [],
   "source": [
    "def fit(epochs):\n",
    "    # Run training, with or without data augmentation.\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        return model.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  shuffle=True,\n",
    "                  callbacks=[checkpoint])\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            # set input mean to 0 over the dataset\n",
    "            featurewise_center=False,\n",
    "            # set each sample mean to 0\n",
    "            samplewise_center=False,\n",
    "            # divide inputs by std of dataset\n",
    "            featurewise_std_normalization=False,\n",
    "            # divide each input by its std\n",
    "            samplewise_std_normalization=False,\n",
    "            # apply ZCA whitening\n",
    "            zca_whitening=False,\n",
    "            # epsilon for ZCA whitening\n",
    "            zca_epsilon=1e-06,\n",
    "            # randomly rotate images in the range (deg 0 to 180)\n",
    "            rotation_range=0,\n",
    "            # randomly shift images horizontally\n",
    "            width_shift_range=0.1,\n",
    "            # randomly shift images vertically\n",
    "            height_shift_range=0.1,\n",
    "            # set range for random shear\n",
    "            shear_range=0.,\n",
    "            # set range for random zoom\n",
    "            zoom_range=0.,\n",
    "            # set range for random channel shifts\n",
    "            channel_shift_range=0.,\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            # value used for fill_mode = \"constant\"\n",
    "            cval=0.,\n",
    "            # randomly flip images\n",
    "            horizontal_flip=True,\n",
    "            # randomly flip images\n",
    "            vertical_flip=False,\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            validation_split=0.0)\n",
    "\n",
    "        # Compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        datagen.fit(x_train)\n",
    "        \n",
    "        steps_per_epoch = len(x_train) / batch_size\n",
    "\n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        return model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=(x_test, y_test),\n",
    "                            epochs=epochs, verbose=1, workers=4,\n",
    "                            callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjLNlernZncm"
   },
   "source": [
    "## Compute $K_z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "Wno8SjkEQJev",
    "outputId": "e4f4f1cd-7b72-4d42-a1ac-77926e8c2809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/1\n",
      "391/390 [==============================] - 57s 145ms/step - loss: 2.5275 - acc: 0.1688 - val_loss: 2.2917 - val_acc: 0.2055\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.20550, saving model to gdrive/My Drive/cifar10_model.001.h5\n"
     ]
    }
   ],
   "source": [
    "fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1303
    },
    "colab_type": "code",
    "id": "iOxufy-hVScU",
    "outputId": "59f2dacd-5bc8-4e1d-e513-c794edf7e10f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7fda9eee4b38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9ee6a390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9ee6a668>,\n",
       " <keras.layers.core.Activation at 0x7fda9ee6a4a8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9ee2d518>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9edfd240>,\n",
       " <keras.layers.core.Activation at 0x7fda9edfdda0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9ed29f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9ecf65c0>,\n",
       " <keras.layers.merge.Add at 0x7fda9ec92518>,\n",
       " <keras.layers.core.Activation at 0x7fda9ebc5588>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9ebc5908>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9eb945c0>,\n",
       " <keras.layers.core.Activation at 0x7fda9eb7c5f8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9eadb2b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9ea5a9e8>,\n",
       " <keras.layers.merge.Add at 0x7fda9eaa84a8>,\n",
       " <keras.layers.core.Activation at 0x7fda9e9d5630>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e9d5ba8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9e97b780>,\n",
       " <keras.layers.core.Activation at 0x7fda9e912c18>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e8e9710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9e8bc4a8>,\n",
       " <keras.layers.merge.Add at 0x7fda9e85c278>,\n",
       " <keras.layers.core.Activation at 0x7fda9e785198>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e785518>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9e702e10>,\n",
       " <keras.layers.core.Activation at 0x7fda9e6c9a58>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e683dd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e595a20>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9e623748>,\n",
       " <keras.layers.merge.Add at 0x7fda9e5af4e0>,\n",
       " <keras.layers.core.Activation at 0x7fda9e5665c0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e504240>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9e4b2278>,\n",
       " <keras.layers.core.Activation at 0x7fda9e4b2320>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e463320>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9e3ddac8>,\n",
       " <keras.layers.merge.Add at 0x7fda9e431438>,\n",
       " <keras.layers.core.Activation at 0x7fda9e35d6a0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e35dc88>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9e2ff2e8>,\n",
       " <keras.layers.core.Activation at 0x7fda9e297cf8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e2717f0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9e1c3588>,\n",
       " <keras.layers.merge.Add at 0x7fda9e1e1358>,\n",
       " <keras.layers.core.Activation at 0x7fda9e10d278>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e10d5f8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9e089be0>,\n",
       " <keras.layers.core.Activation at 0x7fda9e04eb38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9e00bf98>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9df1cb00>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9dfd7e10>,\n",
       " <keras.layers.merge.Add at 0x7fda9df385c0>,\n",
       " <keras.layers.core.Activation at 0x7fda9deed6a0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9de8d278>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9de3a080>,\n",
       " <keras.layers.core.Activation at 0x7fda9de3a400>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9dde8400>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9dd63ba8>,\n",
       " <keras.layers.merge.Add at 0x7fda9ddb5320>,\n",
       " <keras.layers.core.Activation at 0x7fda9dce6780>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9dce6e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9dc04630>,\n",
       " <keras.layers.core.Activation at 0x7fda9dc1ce80>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fda9dbf68d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fda9db49668>,\n",
       " <keras.layers.merge.Add at 0x7fda9db65438>,\n",
       " <keras.layers.core.Activation at 0x7fda9da91358>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7fda9da915c0>,\n",
       " <keras.layers.core.Flatten at 0x7fda9daa3080>,\n",
       " <keras.layers.core.Dense at 0x7fda9da62c88>]"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rg-_yLXV5HC"
   },
   "outputs": [],
   "source": [
    "func = K.function([model.layers[0].input], [model.layers[-3].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1_M1hv2WX5HE",
    "outputId": "53ba5397-bd21-4f7f-e7f2-2c112a4ddb24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kz = 172.2095\n"
     ]
    }
   ],
   "source": [
    "Kz = 0.\n",
    "\n",
    "for i in range((len(x_train) - 1) // 128 + 1):\n",
    "    start_i = i * 128\n",
    "    end_i = start_i + 128\n",
    "    xb = x_train[start_i:end_i]\n",
    "    \n",
    "    activ = np.linalg.norm(func([xb]))\n",
    "    if activ > Kz:\n",
    "        Kz = activ\n",
    "\n",
    "print('Kz =', Kz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Qaqw1Uijm7Zz",
    "outputId": "cf1ee8fc-eded-49f9-8c23-dacd92ce1deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max norm = 36.185123\n"
     ]
    }
   ],
   "source": [
    "sess = K.get_session()\n",
    "max_wt = 0.\n",
    "for weight in model.weights:\n",
    "    norm = np.linalg.norm(weight.eval(sess))\n",
    "    if norm > max_wt:\n",
    "        max_wt = norm\n",
    "\n",
    "print('Max norm =', max_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLjx6DNIZt14"
   },
   "source": [
    "## Find learning rate and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGAd19IiVfm0"
   },
   "outputs": [],
   "source": [
    "K_ = Kz / (2. * 128) + 1e-3 * max_wt  # 128 = Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDV3qER-pEwa"
   },
   "outputs": [],
   "source": [
    "lr = 1 / K_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pVP9Rx6NZ4_U",
    "outputId": "c3e0b27c-c53f-4d01-dd04-48daee3c96ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 1.4106789904732149\n"
     ]
    }
   ],
   "source": [
    "print('Learning rate =', lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e50bGqzCaB7l"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr, decay=1e-3,),  # we will compute this later\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gj0b0hZfaMj0"
   },
   "outputs": [],
   "source": [
    "!rm gdrive/\"My Drive/cifar10_model.001.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7182
    },
    "colab_type": "code",
    "id": "OjKHhypRab0-",
    "outputId": "7cabbb0c-e988-48ef-bea5-b7fa0664db61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "391/390 [==============================] - 56s 144ms/step - loss: 2.3233 - acc: 0.2825 - val_loss: 2.0363 - val_acc: 0.3604\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.20550 to 0.36040, saving model to gdrive/My Drive/cifar10_model.001.h5\n",
      "Epoch 2/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 1.7588 - acc: 0.4484 - val_loss: 1.8160 - val_acc: 0.4314\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.36040 to 0.43140, saving model to gdrive/My Drive/cifar10_model.002.h5\n",
      "Epoch 3/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 1.5709 - acc: 0.5141 - val_loss: 1.5495 - val_acc: 0.5258\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.43140 to 0.52580, saving model to gdrive/My Drive/cifar10_model.003.h5\n",
      "Epoch 4/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 1.4306 - acc: 0.5628 - val_loss: 1.7419 - val_acc: 0.4815\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.52580\n",
      "Epoch 5/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 1.3078 - acc: 0.6082 - val_loss: 1.4518 - val_acc: 0.5656\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.52580 to 0.56560, saving model to gdrive/My Drive/cifar10_model.005.h5\n",
      "Epoch 6/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 1.1969 - acc: 0.6458 - val_loss: 1.4690 - val_acc: 0.5407\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.56560\n",
      "Epoch 7/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 1.0927 - acc: 0.6825 - val_loss: 1.9841 - val_acc: 0.4752\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.56560\n",
      "Epoch 8/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 1.0071 - acc: 0.7143 - val_loss: 1.0809 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.56560 to 0.69270, saving model to gdrive/My Drive/cifar10_model.008.h5\n",
      "Epoch 9/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.9396 - acc: 0.7370 - val_loss: 1.2113 - val_acc: 0.6575\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69270\n",
      "Epoch 10/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.8859 - acc: 0.7544 - val_loss: 1.0367 - val_acc: 0.7157\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.69270 to 0.71570, saving model to gdrive/My Drive/cifar10_model.010.h5\n",
      "Epoch 11/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.8389 - acc: 0.7695 - val_loss: 1.1613 - val_acc: 0.6842\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.71570\n",
      "Epoch 12/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.8031 - acc: 0.7798 - val_loss: 0.9581 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.71570 to 0.73580, saving model to gdrive/My Drive/cifar10_model.012.h5\n",
      "Epoch 13/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.7745 - acc: 0.7899 - val_loss: 0.9351 - val_acc: 0.7518\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.73580 to 0.75180, saving model to gdrive/My Drive/cifar10_model.013.h5\n",
      "Epoch 14/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.7379 - acc: 0.8020 - val_loss: 0.8720 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.75180\n",
      "Epoch 15/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.7196 - acc: 0.8064 - val_loss: 0.9106 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.75180 to 0.75330, saving model to gdrive/My Drive/cifar10_model.015.h5\n",
      "Epoch 16/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.6979 - acc: 0.8142 - val_loss: 0.8715 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.75330 to 0.76450, saving model to gdrive/My Drive/cifar10_model.016.h5\n",
      "Epoch 17/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.6750 - acc: 0.8224 - val_loss: 1.1656 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76450\n",
      "Epoch 18/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.6561 - acc: 0.8273 - val_loss: 0.9042 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76450\n",
      "Epoch 19/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.6375 - acc: 0.8329 - val_loss: 0.7581 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.76450 to 0.79370, saving model to gdrive/My Drive/cifar10_model.019.h5\n",
      "Epoch 20/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.6236 - acc: 0.8372 - val_loss: 0.9670 - val_acc: 0.7394\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.79370\n",
      "Epoch 21/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.6092 - acc: 0.8415 - val_loss: 0.8112 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.79370\n",
      "Epoch 22/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.5967 - acc: 0.8453 - val_loss: 0.7749 - val_acc: 0.7912\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.79370\n",
      "Epoch 23/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.5784 - acc: 0.8513 - val_loss: 0.7701 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.79370 to 0.80220, saving model to gdrive/My Drive/cifar10_model.023.h5\n",
      "Epoch 24/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.5734 - acc: 0.8523 - val_loss: 0.8294 - val_acc: 0.7771\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.80220\n",
      "Epoch 25/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.5602 - acc: 0.8559 - val_loss: 0.8093 - val_acc: 0.7866\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.80220\n",
      "Epoch 26/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.5454 - acc: 0.8625 - val_loss: 0.6713 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.80220 to 0.82700, saving model to gdrive/My Drive/cifar10_model.026.h5\n",
      "Epoch 27/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.5363 - acc: 0.8625 - val_loss: 0.7066 - val_acc: 0.8148\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.82700\n",
      "Epoch 28/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.5276 - acc: 0.8673 - val_loss: 0.7194 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.82700\n",
      "Epoch 29/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.5163 - acc: 0.8705 - val_loss: 0.7272 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.82700\n",
      "Epoch 30/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.5102 - acc: 0.8712 - val_loss: 0.7944 - val_acc: 0.7910\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.82700\n",
      "Epoch 31/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.5026 - acc: 0.8729 - val_loss: 0.8114 - val_acc: 0.7933\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.82700\n",
      "Epoch 32/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.4908 - acc: 0.8765 - val_loss: 0.7172 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.82700\n",
      "Epoch 33/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.4801 - acc: 0.8800 - val_loss: 0.7160 - val_acc: 0.8089\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.82700\n",
      "Epoch 34/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.4770 - acc: 0.8805 - val_loss: 0.6905 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.82700\n",
      "Epoch 35/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.4662 - acc: 0.8844 - val_loss: 0.6953 - val_acc: 0.8196\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.82700\n",
      "Epoch 36/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.4635 - acc: 0.8850 - val_loss: 0.7225 - val_acc: 0.8124\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.82700\n",
      "Epoch 37/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.4549 - acc: 0.8876 - val_loss: 0.6677 - val_acc: 0.8257\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.82700\n",
      "Epoch 38/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.4504 - acc: 0.8895 - val_loss: 0.6287 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.82700 to 0.83240, saving model to gdrive/My Drive/cifar10_model.038.h5\n",
      "Epoch 39/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.4411 - acc: 0.8923 - val_loss: 0.8244 - val_acc: 0.7957\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83240\n",
      "Epoch 40/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.4369 - acc: 0.8917 - val_loss: 0.6162 - val_acc: 0.8397\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.83240 to 0.83970, saving model to gdrive/My Drive/cifar10_model.040.h5\n",
      "Epoch 41/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.4349 - acc: 0.8930 - val_loss: 0.7372 - val_acc: 0.8123\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83970\n",
      "Epoch 42/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.4245 - acc: 0.8968 - val_loss: 0.6385 - val_acc: 0.8338\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.83970\n",
      "Epoch 43/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.4212 - acc: 0.8962 - val_loss: 0.6982 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.83970\n",
      "Epoch 44/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.4164 - acc: 0.8972 - val_loss: 0.6919 - val_acc: 0.8204\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.83970\n",
      "Epoch 45/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.4102 - acc: 0.9015 - val_loss: 0.7018 - val_acc: 0.8148\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.83970\n",
      "Epoch 46/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.4052 - acc: 0.9021 - val_loss: 0.6321 - val_acc: 0.8390\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.83970\n",
      "Epoch 47/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3989 - acc: 0.9046 - val_loss: 0.7031 - val_acc: 0.8174\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.83970\n",
      "Epoch 48/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3957 - acc: 0.9056 - val_loss: 0.6623 - val_acc: 0.8347\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.83970\n",
      "Epoch 49/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3888 - acc: 0.9076 - val_loss: 0.6258 - val_acc: 0.8428\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.83970 to 0.84280, saving model to gdrive/My Drive/cifar10_model.049.h5\n",
      "Epoch 50/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3845 - acc: 0.9077 - val_loss: 0.7398 - val_acc: 0.8143\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.84280\n",
      "Epoch 51/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.3809 - acc: 0.9101 - val_loss: 0.7228 - val_acc: 0.8213\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.84280\n",
      "Epoch 52/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3753 - acc: 0.9114 - val_loss: 0.6260 - val_acc: 0.8361\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.84280\n",
      "Epoch 53/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3716 - acc: 0.9122 - val_loss: 0.6397 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.84280\n",
      "Epoch 54/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3705 - acc: 0.9120 - val_loss: 0.6800 - val_acc: 0.8319\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.84280\n",
      "Epoch 55/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3661 - acc: 0.9140 - val_loss: 0.7109 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.84280\n",
      "Epoch 56/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.3604 - acc: 0.9164 - val_loss: 0.6222 - val_acc: 0.8409\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.84280\n",
      "Epoch 57/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3565 - acc: 0.9186 - val_loss: 0.6513 - val_acc: 0.8344\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.84280\n",
      "Epoch 58/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.3549 - acc: 0.9168 - val_loss: 0.7511 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.84280\n",
      "Epoch 59/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.3498 - acc: 0.9204 - val_loss: 0.6705 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84280\n",
      "Epoch 60/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3464 - acc: 0.9194 - val_loss: 0.7921 - val_acc: 0.8088\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84280\n",
      "Epoch 61/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3443 - acc: 0.9191 - val_loss: 0.6252 - val_acc: 0.8404\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84280\n",
      "Epoch 62/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3441 - acc: 0.9187 - val_loss: 0.6590 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84280\n",
      "Epoch 63/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3343 - acc: 0.9241 - val_loss: 0.7124 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84280\n",
      "Epoch 64/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.3359 - acc: 0.9219 - val_loss: 0.7218 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84280\n",
      "Epoch 65/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.3302 - acc: 0.9244 - val_loss: 0.6545 - val_acc: 0.8358\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84280\n",
      "Epoch 66/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3243 - acc: 0.9280 - val_loss: 0.6568 - val_acc: 0.8344\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.84280\n",
      "Epoch 67/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3281 - acc: 0.9253 - val_loss: 0.6875 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.84280\n",
      "Epoch 68/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3220 - acc: 0.9266 - val_loss: 0.6555 - val_acc: 0.8427\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.84280\n",
      "Epoch 69/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3172 - acc: 0.9286 - val_loss: 0.6314 - val_acc: 0.8404\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.84280\n",
      "Epoch 70/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3153 - acc: 0.9287 - val_loss: 0.7713 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.84280\n",
      "Epoch 71/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.3117 - acc: 0.9300 - val_loss: 0.6564 - val_acc: 0.8406\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.84280\n",
      "Epoch 72/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.3109 - acc: 0.9297 - val_loss: 0.6677 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.84280\n",
      "Epoch 73/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.3093 - acc: 0.9309 - val_loss: 0.6803 - val_acc: 0.8318\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.84280\n",
      "Epoch 74/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.3048 - acc: 0.9316 - val_loss: 0.6507 - val_acc: 0.8434\n",
      "\n",
      "Epoch 00074: val_acc improved from 0.84280 to 0.84340, saving model to gdrive/My Drive/cifar10_model.074.h5\n",
      "Epoch 75/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2990 - acc: 0.9318 - val_loss: 0.6205 - val_acc: 0.8466\n",
      "\n",
      "Epoch 00075: val_acc improved from 0.84340 to 0.84660, saving model to gdrive/My Drive/cifar10_model.075.h5\n",
      "Epoch 76/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.3006 - acc: 0.9335 - val_loss: 0.6589 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.84660\n",
      "Epoch 77/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2984 - acc: 0.9333 - val_loss: 0.6510 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.84660\n",
      "Epoch 78/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2957 - acc: 0.9352 - val_loss: 0.6081 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00078: val_acc improved from 0.84660 to 0.85040, saving model to gdrive/My Drive/cifar10_model.078.h5\n",
      "Epoch 79/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.2922 - acc: 0.9350 - val_loss: 0.6371 - val_acc: 0.8442\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.85040\n",
      "Epoch 80/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.2881 - acc: 0.9370 - val_loss: 0.6524 - val_acc: 0.8409\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.85040\n",
      "Epoch 81/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.2866 - acc: 0.9370 - val_loss: 0.6692 - val_acc: 0.8401\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.85040\n",
      "Epoch 82/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.2817 - acc: 0.9393 - val_loss: 0.6236 - val_acc: 0.8502\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.85040\n",
      "Epoch 83/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.2858 - acc: 0.9380 - val_loss: 0.7395 - val_acc: 0.8335\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.85040\n",
      "Epoch 84/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.2821 - acc: 0.9372 - val_loss: 0.6103 - val_acc: 0.8566\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.85040 to 0.85660, saving model to gdrive/My Drive/cifar10_model.084.h5\n",
      "Epoch 85/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2796 - acc: 0.9387 - val_loss: 0.7351 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.85660\n",
      "Epoch 86/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2753 - acc: 0.9411 - val_loss: 0.6367 - val_acc: 0.8498\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.85660\n",
      "Epoch 87/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2751 - acc: 0.9411 - val_loss: 0.5974 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.85660\n",
      "Epoch 88/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2720 - acc: 0.9414 - val_loss: 0.5992 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00088: val_acc improved from 0.85660 to 0.85700, saving model to gdrive/My Drive/cifar10_model.088.h5\n",
      "Epoch 89/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2706 - acc: 0.9422 - val_loss: 0.6141 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.85700\n",
      "Epoch 90/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2686 - acc: 0.9421 - val_loss: 0.6943 - val_acc: 0.8362\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.85700\n",
      "Epoch 91/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2662 - acc: 0.9439 - val_loss: 0.6519 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.85700\n",
      "Epoch 92/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2639 - acc: 0.9436 - val_loss: 0.6895 - val_acc: 0.8408\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.85700\n",
      "Epoch 93/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2618 - acc: 0.9457 - val_loss: 0.6010 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.85700\n",
      "Epoch 94/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2571 - acc: 0.9458 - val_loss: 0.6049 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.85700\n",
      "Epoch 95/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2561 - acc: 0.9461 - val_loss: 0.6562 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.85700\n",
      "Epoch 96/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2579 - acc: 0.9451 - val_loss: 0.6473 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.85700\n",
      "Epoch 97/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2555 - acc: 0.9463 - val_loss: 0.6204 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.85700\n",
      "Epoch 98/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.2537 - acc: 0.9474 - val_loss: 0.6517 - val_acc: 0.8506\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.85700\n",
      "Epoch 99/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2517 - acc: 0.9470 - val_loss: 0.6165 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.85700\n",
      "Epoch 100/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2500 - acc: 0.9488 - val_loss: 0.6571 - val_acc: 0.8458\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.85700\n"
     ]
    }
   ],
   "source": [
    "fit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcTONBiVajBa"
   },
   "outputs": [],
   "source": [
    "model.save('gdrive/My Drive/epoch-100-resnet20-v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Meg2TMY9CZB"
   },
   "outputs": [],
   "source": [
    "history_100 = model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "A17Khk-Q9N3-",
    "outputId": "c6754799-4ac1-4d3b-8fa0-fe88a14fb08e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 144,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_100.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "8qwn4F8a9R6m",
    "outputId": "15018913-50c5-44c1-b5c7-3dd883621dce"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAEUCAYAAAAsgyAxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XlcVFX/wPHPnYV12AUUBEXUFMh9\nI3LJxEzzqXy0KDV/aVlpmaltlqnpY2X70+pj2aJmaNJiZpSlZqmZu7iLG25ssso6M/f3xwBhLoAy\nDMx8368XL+Zy77n3e6Dwy5lzvkdRVVVFCCGEEEIIB6WxdQBCCCGEEELYkiTEQgghhBDCoUlCLIQQ\nQgghHJokxEIIIYQQwqFJQiyEEEIIIRyaJMRCCCGEEMKh6WwdQHWlp+dV+1ofHzeysgqsGI3tSR/t\nhyP005H66O/vYetQrKa6v4cd4ecNjtFP6aN9cIQ+wrX9HrbLEWKdTmvrEKxO+mg/HKGf0kfH4ijf\nC0fop/TRPjhCH+Ha+mmXCbEQQgghhBDVJQmxEEIIIYRwaJIQCyGEEEIIhyYJsRBCCCGEcGiSEAsh\nhBBCCIcmCbEQQgghhHBokhALIYQQQgiH1mA25hBCiCqpKhQVocnNQcnNRcnJRsnPRykphuISy+eS\nEpTi4n98rRilqPiC86peT8GTz2IOCrZ1rxos7d49uCxfyvmpL4DWMeqgCtHQvPPOmxw4sI9z5zIp\nKioiKCgYT08v5sx59YrtfvhhBe7uBnr3vumS599++3WGDYsjqIH8DpWEWAhR/5jNKOfOoUlPQ5OW\niiY9DSU7C01OWaKbm1PpdTZKbu7fSXBJSa2EoCoKxYNvl4T4Gjh/sxy3d96keMBAjF272zocIcQl\nPPbYE4AlwT1yJJlHH51YrXYDBw6+4vnHH598zbHVJUmIhRB1w2hEycpCk5FekeRq0iu/TkMpP87M\nQDGZqnVb1cUFs6cXZm8f1GbNUT08MXt5o3p6onp6oRoMqM4u4OyE6uSM6uwMTpbX5V/D2RnV6R+v\nPTxQff2s/E2xb+bGTQDQppyQhFiIBmTbti18+eUiCgoKePTRJ9i+fStr1/6C2WwmOjqG0aPH8vHH\n8/D29iYsLJyEhKUoiobjx4/Sp8/NjB49lkcfHcukSU+xZs0vnD+fz4kTxzl16iQTJkwmOjqGRYs+\nZfXqnwgKCsZoNBIXN5xOnbrYrM+SEAshas5ksiS35zItI7nnMsteZ6IpOy5/rZzLhKxz+GdlVXlb\ns7sB1d8fY7MumP0DMAcEWD77B2D29bUkuF5eqJ6emD0tSS/OznXQYXE1zKGhAGhSTtg4EiEaBvcZ\nz+O84ptavWfx4Dvgvbdr3C45+TBLliTg5OTE9u1bef/9j9BoNNx11+3cffe9F1y7d+8evvhiOWaz\nmWHDBjN69NgLzqelpfLaa/9l06YNfPvtciIjo0hIWMaSJcs5f/48cXFDiIsbfk39vFaSEAvh6FQV\nJS8XJTPz78Q2MxNNZqUkNzPTMmpbfj47G0VVq761TofZ1w+CgylpG4nq44vZ378s2Q0sS3b9K5Je\n3N3roMOirphCmgGgPSEJsRANTcuWrXBycgLAxcWFRx8di1arJTs7m9zc3Auuve66Nri4uFz2Xu3a\ndQAgICCA/Px8Tp5MoUWLcJydXXB2dqFt20jrdaSaJCEWoiErKkJ3YB9KZiZKQQFKwfmyz5VfWz5T\n/vp8pa/n5loS3NLSKh+lajSovn6WRPa6tqh+jTD7+mH287Mkur6+qH5+lq/5WF6rHp6gKPj7e5CT\nnlcH3xBRn5iahgCgTTlu40iEaBjOz5jN+Rmza/2+blfRRq/XA3D27Bni4xezYMFi3NzcGDnyrouu\n1VaxaLbyeVVVUVXQaP4udKYoVxFgLZOEWIiGorAQ3Z7d6HbuQLd7J/qdO9Ae2IdiNNb4VqqrK6q7\nO6rBA2O7Dpak1rcsmfVr9Hdi6+uH6udr+ZqnF2ikUqOoAXd3zI0ayZQJIRqw7OxsfHx8cHNz48CB\n/Zw9e5bSagyiXEmTJk04ciQZo9FIXl4e+/fvq6Vor54kxELURwUF6JJ2o9u9A/3OHeh27kB7cP8F\nC81UV1eMHTphbNcec2BjVDc3VHeD5bObe9nnyq8tn3Fzk8RW1BlTSCi6fXstJfHqwzCQEKJGWrVq\njaurG488Mprrr+/A7bcP4fXXX6Fdu/ZXfU9fXz9iYwfw4IP30axZGBERkVWOMlubJMRC1CVVhYKC\nv0uE5eaAUopzylk0aanodu9Ct2sH2oMHUMzmv5u5uWHs3JXS9h0wtrN8mFq1Bp38LyzqN3PTUJTt\n21DS0lADA20djhDiMiqXUevUqUtFxQetVssbb7x7xbaVq0OsXPkLAO+++z8AWrRoWXGuRYuWFV8P\nCQll9GjLvOT77oujSZOg2unIVZJ/TYW4Vvn5aI8dtXwcPYIm9Qya3FyUnBzLYrWcHEsCXPb6UuXE\nPCu9NrsbKO3WA2O79pbkt31HTC1bycYGokEyhVgqTWhTjmOUhFgIUSYzM5OxY0eh1zvRv/8AAgJs\n+/tBEmIhqqKqKOfOoT12BO3RIxckv9pjR9Gkp125uZs7Zi8vzP4BqOGtMHt6ltXItZQNcwsKIE/j\njOrrizEiClOLcEl+hd34OyE+gbFLNxtHI4SoL0aO/D9Gjvw/W4dRQRJiIcoZjWiPJKPbm4R23x60\nyckVya8mN+eiy1WtFnPTEEr69MXUPAxT8xaYwlpgDg7G7OlVsTFEVdMa3Pw9KJIKDMJOSS1iIURD\nYNWEeM6cOezcuRNFUZg6dSrt2rWrOLd69Wo++OADnJycGDRoECNGjLBmKEL8TVVR0tLQ7U1Ct29v\nWQK8F93B/SjFxRde6uKCqVlzSm+IwdQsDFNYi7LkNwxzSCiUlaURQlya1CIWQjQEVkuIN2/ezPHj\nx4mPjyc5OZmpU6cSHx8PgNlsZtasWXz99dd4e3vz4IMP0q9fPxo3bmytcISjKihAd2Afun170VZK\ngDWZmRdcprq4YGwTgaltBMaISIxtIzG1am3ZelYqMghx1aQWsRCiIbBaQrxx40b69esHQHh4ODk5\nOeTn52MwGMjKysLT0xNfX18AevTowYYNGxgyZIi1whGO4Px5dEm70e/ajm7Hdku1hkMHL6jWAGBq\n1pzibtEY20ZgjIzC1DYSU1gLmbcrhDUYDJj9/GTKhBCiXrNaQpyRkUFk5N9b8fn6+pKeno7BYMDX\n15fz589z7NgxgoOD+fPPP+nWTRZbiBoor9O7azv68uT3H6XKyqs1mCKjMEZEYWwbgalNW1SDhw0D\nF8LxmEJC0e3fJ7WIhaiHHnrofp544inatGlb8bUPP3wXLy9v7rnnwums27ZtISFhKbNnz+WZZybx\n8stvXHB++fJ4srOzGTPmoUs+6/DhQzg5OREa2ozp059l6tTpODtffsvnulRni+pUVa14rSgKL7/8\nMlOnTsXDw4OmTZtW2d7Hxw2drvojeP7+9p/0OEwfz5+HXbtg61bYssXyee9eqDzy6+4OMTHQuTN0\n6QKdO6Np3RqnBjLdwWF+lnbOEfp4NcwhzVB2bJdaxELUQ7Gxt/Drrz9fkBCvXfsr77zz4RXb/TMZ\nro51636lTZsIQkObMXPmSzVub01WS4gDAgLIyMioOE5LS8Pf37/iuFu3bnzxxRcAvP766wQHB1/x\nfllZBdV+tr+/B+l2vmrfLvtoNqM5dhTd3j3o9ibhfuQgxu070B47ilLpDyrVzQ1jl26UduhoqdPb\noROm8JYXT3nIPF/HHbg6dvmz/AdH6qMkxReTWsRC1F8339yfRx4Zw7hxEwDYv38f/v7+HDt2lOef\nfxq9Xo+HhwcvvvjyBe0GDbqZlSt/YcuWzfz3v6/j6+uHn18jgoKCMRqN/Oc/M0hPT6OwsJDRo8fS\nuHETvv02gXXrfsXHx4cXXniWzz+PJz8/j5deepHS0lI0Gg3PPDMNRVH4z39mEBQUzOHDh2jd+jqe\neWaaVb8PVkuIY2JieOedd4iLi2PPnj0EBARgMBgqzj/wwAO88soruLq6smbNGu6//35rhSLqIeVc\n5oUVHvYmodu/D6Xgwj98NN7elEbHWBa6te9o2aSiVWuZ7ytEAyK1iIWonhkbnmdF8je1es/B4Xfw\n3u1vX/a8j48vQUHB7N2bREREFL/++jOxsQPIy8tj+vTZBAUFM2vWC/z550bc3Nwuaj9v3rtMmzaL\nVq1aM2XKBIKCgsnLy6Vbtx7ceuttnDp1kmnTnmHBgkV07x5Nnz43ExERVdH+o48+5Lbbbufmm/uz\nZs1qFiz4H2PGPMSBA/uYOXMOPj6+3HnnQPLy8vDwsN6Ag9US4k6dOhEZGUlcXByKojB9+nQSEhLw\n8PAgNjaWu+66i9GjR6MoCmPHjq1YYCfsUHEx+i2b0f+2Bv3OHWj37kF79swFl6h6PaZW11kWukVE\nYYqIwKtnDzL1HjLnUIgGTmoRC1G/xcYO4JdffiYiIoo//viNDz5YwOHDB3nlldmYTCZOnz5F585d\nL5kQnzlzhlatWgPQoUMniouL8fDwZN++PXz3XQKKoiH3ErX8yx04sI+HH34UsGwB/emnHwEQHByC\nn18jABo18uf8+fyGmRADTJky5YLjNm3aVLzu378//fv3t+bjha2YzWj37cVp3RqcfluDftOGC0Z+\nTUHBFPfrj6ltpGXkNyLKMuXByenC+/h7gJ2/zS6EI5BaxEJUz4wbZjPjhtl1/tzevW/i888XEBt7\nCyEhoXh6evLSS7N49dW3aN48jDfeeOWybTWV1uqUrxf7+ecfyc3N5b33PiI3N5cHHhh5hacrFe1K\nS40oiuV+2n+8E1x5LZo1yE51olZoTp1E/9tanNb9itNv69BkpFecM17XhpLeN1Haqw+lXbuj+si7\nAUI4EqlFLET95ubmTnh4Kz7//BNiYwcAcP58PoGBjcnLy2Pbtq2Eh7e6ZNtGjfw5ceIYISHN2L59\nK5GR15OdnU2TJkFoNBrWrfuV0tJSwFJUwWQyXdC+bdsItm3bQmzsAHbs2HrB4r66JAmxuCpKbg76\n39dbRoB/W4vu8KGKc6bAxhQNi6OkVx9Ke99k2dxCCOG4pBaxEPVebOwAZs+ezvTpswAYMmQYjzwy\nhpCQUIYPv48FC/7H2LHjLmo3duw4nn/+aRo3bkJAgGXRbJ8+fXnmmUns3ZvEoEH/IiAggE8+mU/7\n9h15661XL5h68cADD/PSS7NYseIbdDo9zz47DaPRWDedrkRRrT0GXUtqskLdkVa01xUlIwP9X3+i\n/3Mj+j83oNu+raLmr+rmTknMjZT2vomSXjdhuq5Nrcz7dYSfIzhGPx2pj/ZcZaK6P8NL/by9+/dG\nt38fGcdT7WZdgCP9d23PpI/241p+D8sIsbiYqqJNPox+8yZ0mzeh/3MjuuTDf5/WajF27kpJrz6U\n9O6LsVPni+f/CiFEJVKLWAhRn0lCLKC4GN2uHej/3IR+8yb0W/5EU6mGtNngQUmfvpR2j6a0Ww9K\nO3aGSiX0hBCiKlKLWAhRn0lC7KC0e/fgkrDMMgK8fStKcXHFOVPTEIqGDKW0aw9Ku0djahshdX+F\nENdEahELIeozSYgdiaqi3/gHru+8ifMvP1u+pNFgjLweY7fultHfbj0wB1e9lbYQQtSE1CIWQtRn\nkhA7ApMJp1UrcXv3TfTbtgJQEh1D4UPjKe3ZC9XD08YBCiHsnalp+Qhxio0jEUKIi0lCbM+Ki3FZ\nugTX9/9bsSiu+NbbKHhsorxlKYSwuozCDP48s5FBLQZjDpFaxEKI+ksSYjuk5Obg8ukCXP/3Ptq0\nVFS9nsJ7R1I4/nFMZdsrCtEQmFUzR7KT2Za2hdziHFx1brjqXXHRuuKqc8VV74ar1uXCr+tdcdW6\notXIvHdb+yRpPq/+9RKr/v0LnQO7Yvb1lSkTQoh6SRJiO6I5ewbXee/j8tkCNPl5mA0eFIx/nMKH\nxsnmGKJByCjMYFvqX2xL3cLW1C3sSN9OTnH2Vd3LSeOEq94NTydPQj2a0cyzueXDq+yzZxh+Ln4o\ndlIT91Lmzp3L1q1bMRqNPPTQQ/Tv37/i3IYNG3jjjTfQarX06tWL8ePH1/rz2/hadpz6/eRvdA7s\niimkGboD+0BV7aYWsRDCPkhCbAe0hw/h+t7buCz7EqWkBLN/APkTJ1M0ajSql7etwxPikoqMRezO\n2Mm21C2WBDhtKydyj11wTZhXC/qF9qdzYBcC3AIpNBZSZCqi0FhAYWkhRaZCCoyFFa8LjYUUGS2f\nC40FFBqLyCo6x4bTv/PH6fUXxeCuN/ydKJd9NC9Llpt6hOCic6mj70bt27RpE4cOHSI+Pp6srCzu\nvPPOCxLi2bNn8/HHHxMYGMiIESO45ZZbaNmyZa3G0KNJDAAbTv/O450nYw4JRdm5HSU9HTUgoFaf\nJYQQ10IS4gZMc/oU7jOfx/mbBBRVxRjWgsLxj1N01z3g0nD/IW+o8kpyid//BbszdqFRNGgULVpF\ng0bRoFW0aDRay+eyY23ZNZ4GNwoLSnHSOhPiEVKRmHm7+FglxmM5RzmWe5SjOUfJLs7CrJoxq2ZU\n1YxJNVUcm1UVc/kx5kpfN6GqKjqNHr1Gb/ms1VUc6zW6C845afT4eHlQVGBptzcziW2pW9iTmUSp\nubQiNh9nH/qG9qNTQBc6B3ahY2BnfF38aqXfRcYiTualcDz3KMdyj3G80sexnKPszUy6qI2z1pml\ng78hOiimVmKoa127dqVdu3YAeHp6UlhYiMlkQqvVkpKSgpeXF02aWN456t27Nxs3bqz1hNjfzZ/W\nPtfx55lNlJpKL6xFLAmxEKIekYS4ISothVdfxXfGTJSC85S260DB45MoGThY6gWXySnO5tW/XmLZ\ngS/p2rg7wyNGEdvsFnSa2v9P/khOMh/vmseS/YvJL629rTG9nL0vGr0s/2hqCEGv1V/URlVV0grT\nLElvzhGO5R6tSICP5Rwhsyiz1uK7FnqNnqhG19MpsEtFAhzmFW616QsuOhda+rSipU+ri86pqkpm\nUSbHc49WJMjHc4+RVZxFoFvD3UBCq9Xi5uYGwFdffUWvXr3Qlv1+SE9Px9fXt+JaX19fUqpR/cHH\nxw2drnq/Y8q3Tr05vC8fbPmAFOMhgiIsaxh8ctLBTra4tuetustJH+2DI/QRrr6fkhA3MPo/1mN4\nZjIc2I/q50f+nLkUxQ0HjcbWodULZtVM/P4vmLVpOhmF6Xg6efHT8R/56fiPBLo1Jq7NcO5tO5Iw\nrxbX9BxVVVl/ah3zd33AT8d+REWlsXsTJnR6gtta3I5Go8Fstoy4ln+YzZbRVsuxZaTVpJrw8HTm\nXFY+hcZCTuaduGD08uC5/exK33HR87WKluCy0eQQQwjZxdkVyW+B8fxF1+s0OkI8Qmkf0JHmnmE0\n9wqjuWcLGrk2qhi11igalPLR7LJjjaKgqXxM+XUKpeZSSs2lGE1Gy+ey44qvm0spNRspNZXiZtCR\nmZ2LSTXR0rsVUY3a1ZvpCIqi0Mi1EY1cG9E5sKutw6l1q1ev5quvvmLBggXXfK+srIJqXefv70F6\nuuWPw44+3YAP+H5PIm182uAF5CcdoLBv7f3xaCuV+2mvpI/2wRH6CH/382qSYkmIGwglNRXDjOdw\nWb4UVVHgoYc4N+lZVB/fqhs7iB1p23h2/RS2pm7BTefG1O4v8HD7RzmcfYjF+z7jq4NLeXvb67y9\n7XV6BvdmeMR9DAwbXKPErNBYyFcH4/lo14fsO7cXgM6BXXiw3SMMbnHHJUdtq3KlX1Rm1Ux6QVrZ\n2/xHL0iWj+ceY/3JtRXXuuncyxLdMMK8WlS8bu4VRrChqVVGx6vLUX4Z1zfr16/nww8/5KOPPsLD\n4+9/IAICAsiotD17amoqAVaawhAdfCMAG0//jin8FsCyW50QQtQnkhDXd0Yjrgv+h9src9Dk5VLa\nviP5c9/Ap38f1GtIMIxmI2fPn6GpR0gtBmsbGYUZzNk0k8X7PkdF5fbwIcy4YTbBHpYd96IaXc9L\nPV/jhehZfJ/8LYv3fc76U+tYf2odPs4+DLsujuFtR9HWL+Kyzzidf4pPkj7i8z0LyCrOQqfRMaTV\nUB5s94hVRxU1ioZA98YEujeme5MeF50vNBZyKu8kns5e+Lv623XFBFEzeXl5zJ07l08//RRv7wsX\n1zZt2pT8/HxOnjxJ48aNWbNmDa+99ppV4gh0C6Sldys2ndlISS/LnGWpRSyEqG8kIa7HdJv/xOPp\nSej27Mbs5U3eK29QdN/91zxPOKvoHCN/iGPz2U2M6zCB57pPv6qRTVszmo18tmcBL2+eTU5xNm18\n2zKn56vcGNzrkte76lwZdl0cw66LIzn7EIv3LeTL/Yv5364P+N+uD+gc2JURbUdxe6shGPQGALac\n3cz/dr3PiuRvMakmfF18mdhpCvdHPUATQ1BddveSXHWul5wXK8QPP/xAVlYWEydOrPha9+7due66\n64iNjWXGjBlMnjwZgIEDBxIWFma1WG4I6snnexewq/AIAVKLWAhRDymqqqq2DqI6avJ2a0N/e1bJ\nyMB91gu4LlkEQFHccPKnvYjq719xzdX28WReCnHfD+Fg1gEMeg/yS/PoHNiV//X/hBCP0FrrQ224\nUh83nd7AM+unsDczCQ8nT57uOpX7ox6scWJfaiol8dgqFu/7jF9PrEZFxV1v4F/hd3Dg3D62pVm2\num7rG8nYdo8wpPUwXHWu19y3yhr6f6/V4Uh9tOeFK9X9Gf7z551waBkP/zyGF6Jn8fxTCegO7CPj\neGqDr0XsSP9d2zPpo/2QOcT2wmTCZeGnuM+ZiSY7G2NEFHmvvIGx+8VvlV+NPRlJ3LPy35w9f4aH\n2z/KlC5P89Rvk0g4tIybl97I230/4NawQbXyLGs5e/4MMzY8T8KhZQDc02YEz/WYQYDb1c1/1Gv1\n3Bb+L24L/xcn81JYsn8RS/YtYsn+RSgoDGg+kLHtxxET1FOmIwhxlW4I+nsesdQiFkLUR5IQ1xO6\nHdswPPUE+h3bMRs8yJ/9MoWjx4Kudn5Ev5/6jVGr7iWvJJeZN8zhkQ6PAvBBv4+4MbgXU9c/yahV\n9/BQ+/FM6zETJ61TrTy3tpSYSpi3633e2DKX86X5dPDvyJyer9Klcbdae0ZTjxCe7Poskzo/xZbU\nvwh0C6S5l/XeRhbCUTR2b0ILr3DLPOKQkTgjtYiFEPWLJMT1gPPypXg8+hCKyUTRkGGcn/kfzIGN\na+3+3xxazqO/PISKyrzYBdzZamjFOUVRGBExio4BnXnwp1HM2/kef53ZxLz+n9DMs3mtxZBakEr8\n/sWczEupeC6AglJxXPG67LObmzOFhSUoKKw+8RPJ2Yfxc/FjVsxL3Nt2JBrFOqXmtBrtJRewCSGu\nXkxwTxbu/ZTtTXX0wlJpwtjZ/srcCSEaJkmIbcx52Zd4PPYwqsGDnI8/p7T3TbV6/3k732PaH8/i\n4eTJpwMW07Np70teF9koip+GrePpdZNYdvBLbl7ak7f7vs+gFoOv+tmqqrIldTMf757HiuRvL9iV\nrKY0ioYx14/l6a7PWWUHNyGEdUUHxbBw76f85plJL0BzQhbWCSHqD6smxHPmzGHnzp0oisLUqVMr\nthEFWLx4Md999x0ajYaoqCiee+45a4ZSLznHf4HHhEdQPb3IWfYNxg6dau3eZtXMzA3T+GDnOwS6\nNWbJbcuJanT9FdsY9AbevXkeNwb34pn1k7n/x+E8cP1DTL9hNs5a52o/u9BYyLeHE/ho97yKTSWu\n82nD6OvHXrANbvl6ThW14rjiNSq+Pu6cO5cPgJ9rI4IMwdX/Bggh6pXyecR/cAyQWsRCiPrFagnx\n5s2bOX78OPHx8SQnJzN16lTi4+MByM/P5+OPP+ann35Cp9MxevRoduzYQYcOHawVTr3j/OViPB4f\nh+rlRc6ybzG271hr9y4xlTDh14dJOPQVrbxb8+XghGpXkFAUhXvajqBjYGceTBzFR7vn8dfZzczv\n/2mV82lT8k7wadLHLN73GeeKzqFRNAwMG8yY68dyY3CvGi9K8/f3IF1r/6tihXAEQYZgmnuGsTFv\nNyZFahELIeoXq+33u3HjRvr16wdAeHg4OTk55OdbRvv0ej16vZ6CggKMRiOFhYV4eXlZK5R6x+WL\nhZZk2NubnOUrajUZzivJ5Z6VQ0k49BVdG3fn+yE/XVU5tTa+bflx6BruaTOCnenbuXlZT1Ykf3PR\ndaqqsv7kOkatupeui9rxzvY3AZjQcRJ/jdjFp7dapmlIhQYhxA1BN5JTksOO1p5Si1gIUa9YbYQ4\nIyODyMjIimNfX1/S09MxGAw4Ozszfvx4+vXrh7OzM4MGDaqyKLyPjxs6XfU3pKi3tUA/+ggmjgdf\nX5RffsHnGkbF/9nH03mnGbJ8EDtTd3JHmzv4YsgXuOqvvmauPx58cfdCBuyM5ZGVjzAm8T7GdRnH\n67e8jtFs5POdn/Pu5nfZl7EPgE5NOvFYt8eIi4qr0XbIV4yhvv4ca5kj9FP6KG4IvpEv9i9kbYSB\nTqtOgKo2+FrEQgj7UGeL6irv/5Gfn8+8efP48ccfMRgMjBo1iv3799OmTZvLts/KKqj2s+prAWqX\nzz/BY8rjmH19yf5qBabgcLjKOP/Zx0NZB4n7fggpeScYFTmGl3u+Rn62kXyu/ftwa9Cd/PTvCB78\naRTvb3mfxMM/kVaQRl5JLnqNniGthvHA9Q/RObAriqKQl1VKHle/gK5cff051jZH6Kcj9VGS4ssr\nn0e8LtTM5KIiqUUshKg3rDZlIiAggIyMjIrjtLQ0/Mt2WktOTiYkJARfX1+cnJzo0qULSUlJ1gql\nXnD5bIElGfbzIzthJaaoKy9wq4nNZ/7ktoRYUvJO8Gy3aczt9QZazbVt7/xPrX2vY9W/f2VE21Ek\nZx/GXe/O092eY9t9e/kw9mO6NO4m0yKEEFfU1COEUM/m/O6VhVnmEQsh6hGrJcQxMTEkJiYCsGfP\nHgICAjAYDAAEBweTnJxMUVERAElJSTRv3txaodicy4L5eDw5EXOjRpZkOCKy6kbVoKoqXx/6iqHf\nDSa3JJe3b3qfJ7o8abXE1E1tFXa5AAAgAElEQVTvxhs3vcOme7exdUQSk7s8TaBboFWeJYSwTzcE\nxZClKWZ3AGhPptg6HCGEAKw4ZaJTp05ERkYSFxeHoihMnz6dhIQEPDw8iI2NZcyYMdx3331otVo6\nduxIly5drBWKTbl8PA+PZ5/E3Mif7ITvMbVpe833NKtmvtn/DS/8MoPdGTtx07mxaGA8NzfrXwsR\nV62Fd8s6eY4Qwv7cEHQjX+5fzLrmEC61iIUQ9YRV5xBPmTLlguPKc4Tj4uKIi4uz5uNtznX+Bxie\nexqzfwDZX6/E1Pq6a7qfWTWz8sh3vL5lLnszk1BQuLPlv5nS9Vla+bSupaiFEMJ6yucRr20OD8qU\nCSFEPSE71VmJ67z3MEx7FlNgY3ISvsfU6uoTVpPZxHfJX/PGlrkcyNqPRtEwot0IHomcKImwEKJB\nCfVsRoh7U35rdhK2SkIshKgfJCG2AtcP3sUwfSqmxk3I+fp7TOGtruo+RrORrw99xZtbX+Vw9iG0\nipa4NsOZ2Gky3Vt1tPtV+0II+xTdtCdLzy9hf+5hmto6GCGEQBLiWuf63n8xzHweU5MgSzLcoubz\nbUtNpSw/tJQ3t77K0Zwj6DQ6RrQdxYROk6rcLU4IIeq7mKCeLD2whN+dThEntYiFEPWAJMS1yPnb\nBEsyHBRMdsL3mFuE16h9iamEpQeW8Na21zmRewy9Rs+oyDFM6PTEVe02J4QQ9VF0UAwA64JLuScj\nA7WsJKcQQtiKJMS1yPWjeaiKQs6yb2uUDKuqyhf7FvL6llc4mZ+Cs9aZMdeP5dEOEwn2kDcUhRD2\npZlnc4JNBtY1y0dz4hgmSYiFEDYmCXEt0Rw7iv7PjZT07FPjBXQrj6zgibWP4qJ14aF24xjf8XEa\nuzexUqRCCGFbiqJwo7418dptHDq6iRadu9o6JCGEg7PaxhyOxuWreACKht1do3bFpmJe3DgNnUbH\n6mHrmXXjy5IMCyHsXoyfpfb8hrMbbRyJEEJIQlw7VBWXpUtQXV0pue1fNWq6YPd8juUeZUzUWFr7\nXludYiGEaCiiw/oB8HvhHhtHIoQQkhDXCt2WzWiPHaV44GBUg0e122UWZvL6llfwdvZmUpenrBih\nEELUL81aRxOUC7/rT6Gqqq3DEUI4OEmIa4HLsi8BKBpWs533Xt/yMrklOUzu8jQ+Lr7WCE0IIeon\nTy96n3YizamEQ1kHbR2NEMLBSUJ8rYqLcf5mOabAxpT26lPtZoeyDvJJ0keEebXg/qgHrRefEELU\nUz0LAgDYcHq9jSMRQjg6SYivkdPqn9BkZ1M8ZBjoql+048WN0zCpJqZHz8ZJ62TFCIUQon7qqbNU\n5Nlw9BcbRyKEcHSSEF8jl6VLACi6655qt1l/ch2Jx1ZxQ9CN3Bo2yFqhCSFEvRYW0JbGebDx7EaZ\nRyyEsClJiK+Bci4Tp9WJGCOiMEVGVauNyWxi+obnUFB4MWYOimxZKoRwUObQZvQ5Bqml5ziSc9jW\n4QghHJgkxNfA+ZsElNLSGi2mW3pgCUkZu7jrunto59/BitEJIUT9Zm4aSu/jltd/nPrdtsEIIRya\nJMTXwGXZl6gaDcX/Hlat6/NL85nz54u46lyZ2v0FK0cnhBD1mykklN7HLK83nJaEWAhhO5IQXyXt\nkcPot/5Faa8+mBtXb2e597a/TWrBWcZ1mEATQ5CVIxRCiPrNHBJCmwwIKHFiw+nfZR6xEMJmJCG+\nSs5Ly2oPV3Mx3en8U7y/478EujVmfMfHrRmaEEI0CKqnF6q3N73OOHP2/BmO5h6xdUhCCAclCfHV\nMJtx+Soe1c2d4ltvq1aTOX++SKGxkKndX8CgN1g5QCGEaBhMIc3oc6AIgA0yj1gIYSOSEF8F/eZN\naE8cp/i2f4G7e5XX70zbztIDS4hq1I67rqt+eTYhhLB35pBQ+hwqBWQesRDCdiQhvgrOy6o/XUJV\nVaZveA6AmTf8B61Ga9XYhBCiITGFhBKRDn46LzacknnEQgjbkIS4poqKcP72a0xNgiiN6Vnl5auO\nrmTD6d+5pfmt9Gzauw4CFEKIhsMcGooCxOhbcfr8KY7nHrN1SEIIB1T9vYavwpw5c9i5cyeKojB1\n6lTatWsHQGpqKlOmTKm4LiUlhcmTJzN48GBrhlMrnH5ahSY3h4JRo0F75dHeElMJMzc+j06jY3r0\n7DqKUAghGg5TSDMAep0P5DuNZdpEc68wG0clhHA0VkuIN2/ezPHjx4mPjyc5OZmpU6cSHx8PQGBg\nIAsXLgTAaDQycuRI+vbta61QalXFVs3V2Izjk6T5HM05wgPXP0RLn1bWDk0IIRocU0goAL3OOkOQ\nJSG+t+1IG0clhHA0VpsysXHjRvr16wdAeHg4OTk55OfnX3Td119/zS233IJ7NRan2ZqSkYHTr6sp\nvb49pjZtr3htVtE5Xt/yCl7O3kzp+kwdRSiEEA2LOSQEgKgjefi6+EqlCSGETVgtIc7IyMDHx6fi\n2NfXl/T09IuuW7ZsGUOHDrVWGLXK+ZuvUIxGiu+qenT49S2vkF2czaTOT+Hr4lcH0QkhRMOjenph\n9vZGn5JCjyYxnMxP4UTucVuHJYRwMFadQ1zZpVYOb9++nRYtWmAwVF2X18fHDZ2u+hUa/P09ahRf\ntSQsBa0Ww4P3Y7jC/Q9mHmRB0nzCfcJ5+qZJOOucaz8WrNTHesYR+giO0U/po7gcU0gzdMmHiAm6\nnx+OrmDD6d8J9Wxm67CEEA7EaglxQEAAGRkZFcdpaWn4+/tfcM3atWuJjo6u1v2ysgqq/Wx/fw/S\n0/OqfX11aA8ewHfLFor79SdX4wZXuP/jqyZhNBt5rttMcrNKgJJajQWs08f6xhH6CI7RT0fqoyTF\nNWcOCUXZvZMb3KMAyzziuDbDbRyVEMKRWG3KRExMDImJiQDs2bOHgICAi0aCd+/eTZs2bawVQq1y\nKas9XFzFYro/Tq3nx6Mr6dHkBga1qP9VM4QQ9uvgwYP069ePRYsWXXSub9++3HvvvYwcOZKRI0eS\nmppqgwgtyhfWXZ/rgrezt8wjFkLUOauNEHfq1InIyEji4uJQFIXp06eTkJCAh4cHsbGxAKSnp+Pn\n1wDm15rNOH8Vj9nDk+IBgy5/mWrmhT+mApZNOBRFqasIhRDiAgUFBcyaNeuK78LNnz+/XixoNoda\nEmLdyZP0CIrhx6MrSck7QYhHqI0jE0I4CqvOIa5caxi4aDR4xYoV1nx8rdFv+B3tqZMU3jsSXF0v\ne903h5ezO2MnQ1vfTcfAznUYoRBCXMjJyYn58+czf/58W4dSpfJaxNoTJ7ihpyUh3nDqd+5uc6+N\nIxNCOArZqa4anKs5XWL5waUATOnytNVjEkKIK9HpdLi4uFzxmunTp3PPPffw2muv2XTL5PIpE9qU\n48QEWXYA3Xj6D5vFI4RwPFWOECcnJxMeHl4XsdRPBQU4r/gWU9MQSqNjLntZfkke61LWEOEXRQvv\nlnUYoBBC1NyECRPo2bMnXl5ejB8/nsTERAYMGHDFNjWp9lOjxYUdIwBwTT1N7zbReDl78Wfqhgax\nQLEhxHitpI/2wRH6CFffzyoT4gkTJuDp6cnQoUMZOHAgrleYMmCPnH9ciSY/j/MPPASayw+o/3pi\nNSXmEgaG3VaH0QkhxNW54447Kl736tWLgwcPVpkQV7faT82rimjw8/LGnHyErMwCejS5gcRjq9h5\ndD9BhuAa3KduOVL1FHsmfbQf11Ltp8opEytXrmTmzJmcPHmSkSNHMm3aNHbt2nVVgTZE1Z0u8cNR\ny3zoW1tIQiyEqN/y8vIYM2YMJSWWkpB//fUXrVrZdnt5U0go2pQToKpEB90IWKr2CCFEXajWorrW\nrVvTunVrYmJieOONNxg3bhzNmjXjP//5D82bN7dyiLajpKbitOYXSjt2wtSq9WWvKzGV8PPxnwj1\naEaU3/V1GKEQQlxaUlISr7zyCqdOnUKn05GYmEjfvn1p2rQpsbGx9OrVi7vvvhtnZ2ciIiKqHB22\nNnNIKErSLpTMTG4OjWXGhuf4Yt9Chl1X9c6gQghxrapMiE+dOsXXX3/N999/T8uWLXn44Yfp2bMn\nu3fv5sknn2TZsmV1EadNuHy9DMVspqiK0eHfT/1GXkku97YZIaXWhBD1QlRUFAsXLrzs+VGjRjFq\n1Kg6jOjKTKF/L6y7rmNneje9iXUn17AjbRsdAjrZODohhL2rcsrEyJEj0Wg0fPbZZ7z77rv06tUL\nRVFo164d7dq1q4sYbcZ56ZeoOh3Fdwy94nWrjq4EYKBsxCGEEFfFXFZpQpNyAoDxHR8H4L3t/7VZ\nTEIIx1FlQvzdd9/RvHlzAgMDAViyZAnnz58HYNq0adaNzoa0e/egT9pFyc2xqI0aXfY6s2rmx6Mr\n8XPxo1vjHnUYoRBC2I/KtYgBeje9iahG7Vhx5BuO5Ry1ZWhCCAdQZUL87LPPkpGRUXFcVFTEU089\nZdWg6oPyrZqL7rrnitdtS91CasFZbmk+EK2meuWIhBBCXKiiFvFJS0KsKArjOjyGWTUzb9d7tgxN\nCOEAqkyIs7Ozue+++yqO77//fnJzc60alM2ZTDgvX4rZ04uS2CsvNPnh6PeAVJcQQohrYQ4JAf6e\nMgFwe/gQgg1NWbJvEeeKMm0VmhDCAVSZEJeWlpKcnFxxnJSURGlpqVWDsjX977+hPXuG4tvvhCvs\n9KSqKj8cWYGbzp1eTfvUXYBCCGFnVC9vzF7eltJrZfRaPQ+1H0eBsYBPkj6yYXRCCHtXZZWJZ599\nlnHjxpGXl4fJZMLX15e5c+fWRWw2UzFdYtiVp0sczDrAkZxkbmtxO646x9qwRAghapspJBTdkWRQ\nVSir2DOi7She++sVPt49j3EdJsjvWiGEVVQ5Qty+fXsSExNZuXIliYmJrFq1yu5HiHXbtmD29sbY\n/cqL5FaVTZcYKNMlhBDimplDQlEKzqOcO1fxNYOTB/8XOYaMwgyWHlhiw+iEEPasyoQ4Pz+fxYsX\ns3jxYhYtWsSbb77JhAkT6iI221BVtCdTMDULqxihuJwfjqxAp9HRL7R/HQUnhHBUSUlJrFmzBoA3\n33yTUaNGsWXLFhtHVbsq1yKu7MF2D+OkceKDHe9gMptsEZoQws5VmRBPnDiRAwcOkJCQwPnz51mz\nZg0zZsyog9BsQ0lLQykqqqiJeTmn8k6yI307MUE98XbxqaPohBCOavbs2YSFhbFlyxZ2797NtGnT\n+O9/7atG7z9rEZcLdG/M0NZ3cyQnmR+P/WCL0IQQdq7KhLi4uJgXX3yR4OBgnn76aT7//HNWrVpV\nF7HZRPnIhKmKhPjHY5bNOKS6hBCiLjg7O9O8eXN++eUX7rrrLlq2bIlGU+Wv8Abln7WIKxvXwfLO\n5Hvb367TmIQQjqFaVSYKCgowm81kZWXh7e1NSkpKXcRmE+UrnE1lJYAu54ey3elubT7I6jEJIURh\nYSGrVq1i9erV3HjjjWRnZ9tdCcyKWsT/mDIB0Nr3Om5pfitbUjfz55lNdR2aEMLOVZkQ33777Sxd\nupRhw4YxcOBABg0aRKMr7NzW0GnKkn1z2UjFpWQVnWPDqfV0CuhME0NQXYUmhHBgkyZNYsWKFTzx\nxBMYDAYWLlzI//3f/9k6rFp1qVrElY3vULad8w4ZJRZC1K4qy67FxcWhlC0ui46OJjMzk7Zt21o9\nMFv5e4T48lMmfj6eiEk1MbDF4LoKSwjh4Hr06EFUVBQGg4GMjAyio6Pp1KmTrcOqVZeqRVxZ9ybR\ndA7sQuLRHzicdYiWPq3qOEIhhL2qcoS48i51gYGBREREVCTI9qj8rTrzFaZM/HCkbHe6MJk/LISo\nG7NmzWLVqlVkZ2cTFxfHokWL7HKBsykk1DKHWFUvOmfZzvlxVFQ+2PmODaITQtirKhPitm3b8vbb\nb/Pbb7+xcePGig97pUk5gdnHB9XD85LnC0oLWJOymlberWnl07qOoxNCOKq9e/cybNgwVq1axZ13\n3slbb73F8eMXz7Vt6C5Vi7iygWG3EebVgqUHlpBWkFbH0Qkh7FWVUyb27dsHcEG9S0VRiI6Otl5U\ntlJWg9jY8vKJ7rqTayg0FsrosBCiTqllI6Zr165l4sSJAJSUlNgyJKuoXIvY6Od30XmtRsvD7R/l\n6d8m8fHuD3m2+wt1HaIQwg5VmRAvXLiwLuKoF5SMDJTCwivWIJbd6YQQthAWFsbAgQPx9fWlbdu2\nfPPNN3h5edk6rFpnblppYV2HS8+RjmsznLmb/8MnSR/xWKdJGPSGugxRCGGHqkyI77333kvOGV68\neHGVN58zZw47d+5EURSmTp1Ku3btKs6dOXOGSZMmUVpaSkREBC+++GINQ699VdUgNpqN/HRsFY3d\nm9AhwL4Wswgh6rfZs2dz8OBBwsPDAWjZsiVz5861cVS170q1iMu56lwZff1YXv3rJZbsW8iD7R6p\nq/CEEHaqyoS4/K05sNQk3rRpE25ublXeePPmzRw/fpz4+HiSk5OZOnUq8fHxFedffvllRo8eTWxs\nLDNnzuT06dMEBdm2hFn5ymZz6KUT4j/PbORc0Tnuj3oAjWJfBfGFEPVbUVERv/76K2+//TaKotCh\nQwdatmxp67Bq3ZVqEVc2Omos725/i3k73+f+qAfRaar850wIIS6ryqyuW7duFR8xMTFMnjyZbdu2\nVXnjjRs30q9fPwDCw8PJyckhPz8fALPZzNatW+nbty8A06dPt3kyDKApG5EwNb10QvzDkRWAVJcQ\nQtS9adOmkZ+fT1xcHHfddRcZGRk8//zztg6r1pUPSFyuFnE5P1c/4toM50TecVYkf1MXoQkh7FiV\nf1L/c1e6M2fOcPTo0SpvnJGRQWRkZMWxr68v6enpGAwGzp07h7u7Oy+99BJ79uyhS5cuTJ48+SrC\nr13ak5evQayqKquOrsTTyYuYoJ51HZoQwsFlZGTwxhtvVBzfdNNNjBw50oYRWYfq5Y3Z0+uytYgr\ne7j9o3y2ZwHv7fgvd7T8t12XBBVCWFeVCfGoUaMqXiuKgsFg4NFHH63xg9RKNSVVVSU1NZX77ruP\n4OBgxo4dy9q1a+nTp89l2/v4uKHTaav9PH9/jxrHSOppAHw7RoD3he23ndnGyfwUhl8/nKDGvjW/\ntxVcVR8bGEfoIzhGP6WP16awsJDCwkJcXV0BKCgooLi42GrPsyVzSCjao0cstYivkOSGebVgUIt/\nsSL5G34/9Rs9m/auwyiFEPakyoT4119/xWw2o9FYZleUlpai1+urvHFAQAAZGRkVx2lpafj7+wPg\n4+NDUFAQoWVvjUVHR3Po0KErJsRZWQVVPrOcv78H6el51b6+nE/yETRe3mSWauEf7Rdvs8x/7hs0\n4KruXduuto8NiSP0ERyjn47UR2slxXfffTe33norUVFRAOzZs4fHH3/cKs+yNVNIKLo9u1HOnUO9\nROm1ysZ3mMCK5G94b8fbkhALIa5alXOIExMTGTduXMXx8OHD+fHHH6u8cUxMDImJiYDlF3dAQAAG\ng6U0jk6nIyQkhGPHjlWcDwsLu5r4a4+qok05cdkKE6uOfI+z1pmbQm+u48CEEAKGDh3KkiVLuOOO\nO7jzzjv58ssvOXz4sK3DsoqKWsQnq5420SmwC9FBMfx6YjV7M/dYOzQhhJ2qcoT4k08+Yf78+RXH\nCxYsYMyYMQwYMOCK7Tp16kRkZCRxcXEoisL06dNJSEjAw8OD2NhYpk6dyjPPPIOqqrRu3bpigZ2t\nKOfOoRQUVNTArOxITjL7zu3llua3Sr1LIYTNNGnShCZNmlQc79q1y4bRWE95LXjNiRPQvmOV14/v\nMIGNp//g/R3/5d2b51k7PCGEHaoyIVZVFQ+Pv98CNBgM1V64MGXKlAuO27RpU/G6WbNmLFmypLpx\nWl1FDeJLlFxbdWQlINUlhBD1S+W1GfakvBaxLvkQ1dmLr1+zW2jtcx0Jh5YxtfsLBBmCrRugEMLu\nVDllIioqiokTJ/LFF1+wePFiHn744Yo5bPakvMTPpXapW3X0ezSKhv7Nb63rsIQQ4rLstapCadfu\nqDodzl8vtyysq4JG0TCuwwSMZiP/2/VBHUQohLA3VY4QP//883z33Xfs2rULRVH417/+VeV0iYao\nfFek8pGJcmkFafx19k96BN1AI9dGtghNCOHAevfufcnEV1VVsrKybBCR9an+/pQMGITz99+i274V\nY6cuVbb5d+u7eOnPWXy+5xMmdX4ST2f729ZaCGE9VSbEhYWF6PV6pk2bBsCSJUsoLCzE3d3d6sHV\npfIpE+aQC+cQJx77ARWVgTJdQghhA1988YWtQ7CJwhGjcP7+W1wWfUZ+NRJiZ60zD7Z7mNmbZvD5\n3k95tKN9VuAQQlhHlVMmnn766QvKpxUVFfHUU09ZNShb0Jy0bEDyzyoTsjudEMKWgoODr/hhr0r7\n9MUUEopLwlco+dUr2TcqcjTuegPzdr5HkbHIyhEKIexJlQlxdnY29913X8Xx/fffT25urlWDsgVt\nygnMHp6oXt4VX8sryWX9yXVENWpHqGezK7QWQghRqzQaiu4ZgVJwHudvEqrVxMvZm/ujHiC14CxL\n9i+ycoBCCHtSZUJcWlpKcnJyxfHu3bspLS21alB1TlXRnDhhWVBXaa7eL8d/psRcwq1hg2wYnBBC\nOKaie0agajS4LPq02m0ebv8orjpX3tn2JqUmO/u3SghhNVXOIX722WcZN24ceXl5mM1mfHx8mDt3\nbl3EVmeUrHNozudT+o+Sa6uOfg/AwLDBtghLCCEcmjm4KSV9++G8+ie0e5IwRVZd4SjALYCREf/H\n/3Z9wLKDX3Jv25F1EKkQoqGrcoS4ffv2JCYmsnz5cp555hkCAgJ45JFH6iK2OqMtnz9caVOOYlMx\nPx//iVDP5kT4RdoqNCGEcGhFI/4PAJfFn1W7zbgOE3DSOPH2ttcxmo1WikwIYU+qTIh37NjBCy+8\nwODBg3nxxRe56667WLNmTV3EVmc0J8prEP89T/iPU7+RX5rHwLDb7LbWpxBC1HclsbdgCgjEZVk8\nFBZWq02QIZi4NiM4mnOEbw9Xb/6xEMKxXTYhnj9/PgMHDuSJJ57A19eX5cuXExoayqBBg9Dr9XUZ\no9VpU8prEP89ZWLlkfLpElJdQgghbEavpzhuOJqcbJxXflftZo91mohW0fLW1tcwq2YrBiiEsAeX\nTYjfeust9Ho9L730EhMnTqRZs2Z2O1KqKa9BXDaH2Kya+fHoShq5NqJr4+62DE0IIRxe4XBLpSOX\nRdWfNtHMszlDW9/Ngaz9/FA2wCGEEJdz2YR47dq1DBo0iOnTpxMbG8v7779vf9Ulymj/UYN4y9m/\nSC9M45bmA9FqtLYMTQghHJ45rAUlPXvjtOF3tMmHqt1uYufJKCi8ufVV1GpsAS2EcFyXTYj9/f0Z\nO3YsiYmJzJkzhxMnTnDq1Ckefvhh1q1bV5cxWp32xAnM7gZUbx+gcnUJmS4hhBD1QVH5KPHihdVu\nE+7dijtaDmF3xk5WH0+0VmhCCDtQ5aI6gK5du/Lyyy+zfv16+vTpw3vvvWftuOqOqqJJOWGZLlE2\nJWT18UTcdG70bNrHtrEJIYQAoHjgYMw+Prh8uRhKSqrd7vHOUwB4Y+tcGSUWQlxWtRLicgaDgbi4\nOJYuXWqteOqckpONJi+3YrpEekE6B7L2071JNC46FxtHJ4QQAgAXF4qGxaHJSMfppx+r3SzCL5Jb\nw25ja+oW1p+yr3c3hRC1p0YJsT0qrzBhLkuIN57+HYCY4J42i0kIIcTFymsSu9Zg5zqAJ8pGid/c\n8motRySEsBcOnxBrUso35bAkxL+f+g2AG4JutFlMQghRGw4ePEi/fv1YtGjRRec2bNjA0KFDufvu\nuxvMNDhTm7aUdumGfs0vaMoGM6qjQ0An+ob244/T69l0ZqMVIxRCNFQOnxBry0qumcpKrm04/Ttu\nOnfa+3e0ZVhCCHFNCgoKmDVrFtHR0Zc8P3v2bN555x2WLFnCH3/8weHDh+s4wqtTNGIUiqrisuTi\nJP9KJnV+GoA3t8y1RlhCiAbO4RNiTaUpE2kFaRzMOkD3Jj3Qa+1r8xEhhGNxcnJi/vz5BAQEXHQu\nJSUFLy8vmjRpgkajoXfv3mzc2DBGTotuH4LZ4GFJiE2marfr1qQ7Nwb3Yk3KL2xP3WrFCIUQDZHD\nJ8TaE+W71DWT+cNCCLuh0+lwcbn0wuD09HR8fX0rjn19fUlPT6+r0K6NuzvFQ4ahPXUSp7W/1Kjp\nE52fBODNba9ZITAhREOms3UAtqY5mYLq5o7q68sfSesBmT8shBCX4uPjhk5Xvc2K/P09rBfIY4/A\n5wvwWroY4v5d7WZ3NhpE9LZofjy6kjPmo7QLbHfNoVi1n/WE9NE+OEIf4er76fAJsTblBKaQEFAU\nmT8shHAIAQEBZGRkVBynpqZecmrFP2VlFVTr/v7+HqSn5111fFUKbY13VDt0K1aQmXQYNTCw2k0f\naz+JjSeH8cLPM5l/y6fXFIbV+1kPSB/tgyP0Ef7u59UkxQ49ZULJyUaTk42p0vzhHkHRMn9YCGHX\nmjZtSn5+PidPnsRoNLJmzRpiYmJsHVb1KQpFw+9DMRpxif+iRk1vDu1PO/8OfJf8NYeyDlopQCFE\nQ2PVEeI5c+awc+dOFEVh6tSptGv399tTffv2pXHjxmi1lrffXnvtNQJr8Fd+bSgvuWYOCa2YP3xD\nkMwfFkI0fElJSbzyyiucOnUKnU5HYmIiffv2pWnTpsTGxjJjxgwmT54MwMCBAwkLC7NxxDVTPPQu\nDC9Ow2XxZxQ+NrFip9GqKIrCE52f5P4fh/P2ttd59+Z5Vo5UCNEQWC0h3rx5M8ePHyc+Pp7k5GSm\nTp1KfHz8BdfMnz8fd3d3a4VQpfJNOUwhzfjjlGX+cEywzB8WQjR8UVFRLFy48LLnu3btetHv5IZE\n9fKmePAduCxdgv6P9foCaCYAACAASURBVJTe2KvabW8NG0Qb37YsP7iUKV2eoblXw/pjQAhR+6w2\nZWLjxo3069cPgPDwcHJycsjPz7fW466K9mRZQhwayobTv+OuN9CuUQcbRyWEEKI6ikaMAsBl0Wc1\naqdRNEzsPAWTauKd7W9aIzQhRANjtRHijIwMIiMjK47Ly/oYDIaKr02fPp1Tp07RuXNnJk+ejHKF\nt7xqsroZqrnKMOMsAEWt/Tn46wEGtBxAUGPfKhrVH46wYtQR+giO0U/po6htpd2jMbZshfPK78jP\nOofqU/3f37eHD2Hu5jl8uX8xkzo/RbBHUytGKoSo7+qsyoSqqhccT5gwgZ49e+Ll5cX48eNJTExk\nwIABl21f3dXNUP3VlJ4HDuMMrCiw7NDUpVF0g1mF6QgrRh2hj+AY/XSkPkpSXIcUhaLhozDMfB6X\nr+IpfPCRajfVarQ83mkyj68Zx3s73mZOz1etGKgQor6z2pSJf5b1SUtLw9/fv+L4jjvuwM/PD51O\nR69evTh4sO5X+2pSTqC6uvJH3k5A5g8LIURDU3T3vah6vWXaxD8GXqoytPXdhHiEsmjvZ6QWpFop\nQiFEQ2C1hDgmJobExEQA9uzZQ0BAQMV0iby8PMaMGUNJSQkAf/31F61atbJWKJelPXkCU4jMHxZC\niIZKbdSI4ltvQ7dvL7qtf9WorV6r57GOT1BkKuLDHe9aKUIhRENgtYS4U6dOREZGEhcXx+zZs5k+\nfToJCQn8/PPPeHh40KtXL+6++27i4uLw9fW94nQJa1DyctFkZXEmLICDWQfo3qSH1B8WQogGqGj4\nfQC4LP68xm3j2gynsXsTPkn6iMzCzNoOTQjRQFh1DvGUKVMuOG7Tpk3F61GjRjFq1ChrPv6KymsQ\nrwu3fAuk/rAQQjRMpb1vwhTaDJevl3N+1kuohurP43bRuTC+wwSm/fEs83e9zzPdp1kxUiFEfeWw\nO9WV1yBe28iy0EfmDwshRAOl0VB070iUgvM4J3xV4+YjI+6nkWsjPtz5Hi9vns3Z82esEKQQoj5z\n2IRYk3IcgPVOp2T+sBBCNHBFccNRNRpcFtesJjGAm96Nl3u+jl7rxBtb5tJpYSQP/zyaLf/f3r3H\n51z/fxx/XIedz5trR3MaGcuwoYRRDDkV37AiSUWOFX2Fny86oVIq+upAB1JNLClyCJO+zYaxyiFM\n2ezANnZk23X4/P64uDKnHHa8rte9226u6/qc3q9d27vnPtf78/5kJ10xQ5IQwjrZbCDWpKeT7QpH\nDFkyflgIIeo4U2AQ5T16YrcvGc3vv9309gOaDmT/iEMs6PoOTT2bEXd0NX3ietBrdTdW/fElZcay\nKmi1EKK2sOFAnMaOhubHMn5YCCHqvtJh5utSnD756Ja2d7FzYUTY4+wYuou4B77n/sb9+DU3hQlb\nx9B2eUsZTiGEFbPZQKxOT2N7iPnOdzJ+WAgh6r7y6F4YGzTCacWnOL1/69OoqVQqOgdF8dn9X5A0\nLIXxbZ5Bb9JXGE6xOztRhlMIYUVsNhBr0k+wI0SNi50rrXVta7o5QgghbpdWS0HsGoz+AbjOmnFb\nofiiBu4NmX3Py5bhFM087yDu6Gr6xkXTYWkHGU4hhJWwzUBcXMzp8jMc9tBzd0BHtOpqu4O1EEKI\nKmQMaUbBN99j9PM3h+IP3quU/V4cThE/NIG4B76nT+P+JGclW4ZTxKdvq5Tj/BOTYmLOLzMZvO4B\nfs+9+bHSQoirs8lArDmZ/vf44SAZPyyEENbEGNKMgrXrzaH4P9MrLRTD38MpPr1/JamTUpnQ9lmK\nygt5dMNQtqdtrbTjXI1JMfF8/DP8d/+77Di5neivo3g5YTbn9Oeq9LhC2ALbDMTpJ4hvZH7cKVDG\nDwshhLW5IhR/+N9KP0Yjz0bM6vgSy+//ChUqRvwQw7a0Hyv9OPB3GP780GeE69qwrNcKgtyCWbRv\nIVGxd1d5GBfC2tlkIFanpRHfCFxVDoTrZP5hIYSwRubhExdC8cxpVRKKAe5t0J3lfcyh+LEfHmZb\n2pZK3b9JMTElfpIlDH/dfy39Qx7gp6G7mND2WTKK0hn6/UDG/fgUuedzK/XYQtgKmwzEORmHOKyD\nu93DZfywEEJYMWPTy0LxR0uq5Djdgu9jRZ9Y85niDQ+z9cTmStmvSTExeftEVh5aTmtdW1b3/xYv\nR2/AfEORWR1fYvPgHbTRtWX1kVg6fRHJV4dXygwYQtwkmwzE/yv6FYB7grvVbEOEEEJUOUso9vXD\n9f9ewHHp+1VynK7B9/J531Vo1Boe++ERfjyx6bb2Z1JMPLd9Al8cXkEbXVu+7r8WT0evK9ZrVS+c\nH/61jVc6zafMWM6kbWN5aN0Ajucfu63jC2FLbDIQ/8yfANzTrHcNt0QIIUR1uDQUu82YWmWhOKp+\nNz7vYw7FI38Yxpa/Nt7SfowmI89uH8+Xhz83h+EB3141DF+kUWsY3XocPz+cRM+GvdmZsYOusR15\ne+8Cyo3lt1qOEDbDJgPxTx5ncdWrCPeT+YeFEMJWGJvdUTEUL/ugSo7TpX7Xv0PxxmFs/uuHm2vn\nhTD81eGVtPWN4OsB3+Lh4HlD29Z3C2ZFn1iW9VqOh4MncxNfIvrrKHZnJ950HUaTkayiLFJO72Nb\n2hYKyvJveh9C1BU2N4D2VO6fHPY20ivPW8YPCyGEjbkYij0f7IPb9H8DUPrEmEo/Tpf6XVnZ92uG\nrR/M4xuH83Hvz+nV6P5/bp/JyDPbx7Hqjy+J8I0ktv83NxyGL1KpVPQPeZCo+t14OWEOyw9+TL+4\nnoy88wn+767ZuDt4UFxeRFZJFlklmWQVZ3LqXPaFx1lkl2SSXZLNqXPZGBWjZb8BLoF80PMT7g7o\neNPfDyFqO5tLhLsOfg9AFxrXcEuEEELUBGOzO8hfu+GSUKyi9InRlX6czkFRfNF3NcPWD2bUxuEs\n67WC3o37XLtdJiOTto3l6yNfEeEbyar+a3F38Ljl43s4eLKg29s81Hwoz8dP4pPfl7L6yCpMiokS\nffE1t7NT2+HvEkBb30ga+TTAW6vDqBj59PdlDFzbh2kdZjIx4jnUqqr5kLncWM7ifW+zNW0LM++e\nQ8fATlVyHCEuZXOB+Jf0eAA6u8t0a0IIYauMze4g/5v1eA7si9v05wGqJBR3CurCF31X88j6h3hi\n06Ms7bWc+xv3vbI9l4ThSL92xPb75rbC8KXuDujI1iE/syh5IV8e/hw3e3cCXAIIcAnEz8WfAJdA\nAlwC8HcNxN85AB8nH0vY1encyMkpAuCBpv9izObHeTXxRX7J/Jn3enxEPad6ldLGi/ad2suz2ydw\n6MwBAB5c24dnIqbw7/bTsdPYVeqxhLiUSqkjc7Nc/IW8EZf+Al+u05KmZJWe5kT99zEOfqSymlft\nrlejtbCFGsE26rSlGnU6t5puSpW50fewLr3fmiN/4DmwL+qc0xTNe+Omhk/cTJ0Jmf/j4e8fotxU\nxtKey+nTpJ9lmdFkZOK2p1l9JJZIv/bE9ourtDB8uy6vMe98HhO2jmZr2hb8XQL4IPrjSjmDW6Iv\n4bWkV/nw1/9iUkw82nIk/Zo8wL9/eo60wr+I8I1kSfQyGns0ue1jXa4u/bzeKluoEW6vH7api+pO\nlWRzVDlNlzRQNaj8XyohhBB1i/GO5uTHfY9J54vb9H/jPnIY6qzMSj9Ox8BOfNVvDfZqB57cPIL1\nx78zH99kZMLWMZYwvKp/5Z0Zrgo+Tj6s7Ps1/+n4EjnnTjPw274s3PMGJsV0y/v86WQ8XWM78n7K\nYhq6N+KbB9bzZrd3ubdBd7YP+ZmH7hhK8um93Leqs8yxLKqMTQXiXzJ/BqDbX2Bq0KBmGyOEEKJW\nMDYP5ex3myi/+x4cNnyHV6f2OC77EIzGf974JtwdeI8lFD+1+THWHfuG8VtHs+boKtr5dWBV/29w\ns3ev1GNWBbVKzcS2z7L2wR/wdw5gXtLLDP1uIKfPnb6p/eSXnuXZbeN5aN0AMorSmdD2WeKHJtAp\nqItlHTd7d/7b4yOW9FiKWqVm0raxjNnyuMx4ISqdTQXi/2VcCMTpGkx+/jXcGiGEELWFqUkIBWs3\nUPTWItBocJv+PJ79eqI5eKBSj3N34D181T8OB40jT25+jLijX9Pe/y5i+8fViTB8qbsC7mbb0J+J\nbtiLHSe3c9+qTvyc8dMNbft96jo6f9WBLw6vIMynFRv/tY1ZHV/CSet01fX/dccQtg35mfb+d7H2\nWBz3xnZiV+YvlVmO1TEpJvJLz3K8IJWkjCS2ntjM1398xdqjazhZlF7Tzat1bGoM8T1fRHLq9DFO\nf9WIol0pldm8amcL44FsoUawjTptqUYZQ1z332/VqVO4zpqG4zdrULRazo9/hpLJU8GpYli7nToT\ns3YxbP1gWvi05Iu+X9faMHwjNZoUE0v2L+bVxDmYFBPPt5vGc5H/RqPWXLHuqZJspu18nvXH1+Gg\nceD5dtMY12bSDV8wZzAZeGvP67y193UAno2YwpR2027rgrvb/XktM5aRUXwSFAVUKgBUF/+79Pll\ny8A8RZ7BZMBg0mMwGdGb9BgVA/oLzw0mvfk1kwG9yWBZV2/SU6wv5mzpGfNX2VnOlp7hTOkZ8i88\nzi/Lv+5QlkCXIDoE3EUH/7vpEHA3LX3urPPT0d5OP1ylgXju3LmkpKSgUqmYMWMG4eHhV6zz5ptv\nsn//flasWHHdfd1uID5Vkk2rz+6gzxH4Jr0bBWvW3fD+aqO6/j+cG2ELNYJt1GlLNUogtp73237r\nZlynTkaTnoaxUWOKFryDPqqbZfnt1nlOfw5HrWOVTV9WGW6mxt3ZiYzZPIqTxel0CerKf6OX4ufs\nB4CiKHx5+HNm//J/FJTlc1dARxZ2W0xTr2a31K7ErF2M+/FJ0ovSiPRrx5Iey2jkcWvTqd7s+3iq\nJJuk7ET2ZCexOzuRX3P2U26qHXcD1Kq1eDl44+3ojaejF16O3ng5eBHk5Y+j4oqngxfF+mKSsnaR\nlL2L3PM5lm1d7FyJ8GtHB39zSG7n3/6m/lBTFIW80rwL81lnkFmcSXZJJlklWZgUE45aJxw1Djho\nHHHUOuKgdcRJY/7XQeOAk9YJB8ty87pNPENuqg230w9X2Z8CSUlJnDhxgtjYWFJTU5kxYwaxsbEV\n1jl27Bi7d+/Gzq7qp1K5dPywsbGMHxZCCHF95d17cuanRFxen4vTB+/h+dAASoc8TPGLc1F8fG57\n/852zpXQytqjvf9dbB2yk0nbxrLprx+4L7YTS6KX0sCtIVN2PMPOk/G42rnxWtRbPBY26rb+ELgr\n4G62D/kfU3+aTNzRr7lvVWfmRy1g8B0xljOxlUFv1HMw73f2nDKH3z3Zu0krOmFZrlFpuLNeOC18\nWqJRmc+IK4qCgvlco4JS8flVHtup7dCqtWjVdmjVGrRqO/NrKu2F17V/v3ZhuVatxdXOFS/HC+HX\nwQtvR29c7FyvWv/loX9cm4koisKfhcdJytrF7uxEErMS2Hkynp0n4wHzWPGWPneaA3LA3bSq15qC\nsnzzDV2KM8gsMQfezOJMMksyOVWSRZmxrNK+9wCh3i34Kebm77J4K6osECckJNCjRw8AQkJCKCgo\noLi4GFdXV8s68+fP57nnnmPx4sVV1QwLy/jhv8AUJYFYCCHEDXBxoeTFVyn712BcpzyD46ovsf9x\nE8UvzoXxlT9vcV3n5ejN8vu/4oNf3+OlhFkMXvcADhoHSo2lRDfsxetRCwlyq18px3J38OD96GV0\nbxDNCz9NYcLWMWxL28LrUQtveaaOvPN57D2VxO7sJPZkJ7Hv9F7OGc5Zlns7etOr0f208+tAe/+7\naO3bFhc7l0qpp7qpVCqaeITQxCOEmNBhAJwpzWNPdhJJWYkkZe9i3+m9/J77Kx///tG194MKPxd/\nWvqEEeASRKBrIP4ugQS6BlrmuNaotZQaSikzllJqKKPUeJ4yQymlxrILr12+rIxSYymRfu2q69tR\ndYE4NzeXsLAwy3Nvb29ycnIsgTguLo4OHToQFBRUVU2o4JfMnbjhQNvsMs4FSyAWQghx4wzhbcj/\nYStOS9/HZf4ruE98GtZ+jfrVBZiahNR082oVlUrF060n0N7/LsZsHkWJvpi373uPgU0fqtSztxcN\nbh5De/+7GPvjk8QdXU1SViIdAu5CbzKgN5ajN+nNj03l6I36C2NwLzw36TFhpFRfht5UzpnSM3/X\ngYpQ75a08+9A+wtfTTyaVkkNtYW3ow89G91Pzwu3GS83lvNrzn4Ss3bxx9lDeDv6EOgSSMCFsBvo\nGoSvs1+dH3sM1XinukuHKufn5xMXF8cnn3zCqVOnbmh7Ly9ntNorB+hfy6XjR7KKsjiWf5Q+xhC0\nplTcW4WCFYzzs+axihfZQo1gG3VKjaLO02o5//QEyvoOwHXaFBy2bML7fx05N3kq556eAI6ONd3C\nWiXSrz2/PLL3wvjRqv3eNPJozHcDN/HmntdYuPcN4o6uvup6WrUWe7X9hSEIWuw09jho7XG2c8ZO\n7UG4rg3t/e+inV8HIv3a1eo5oauDvcaedv4daOffoaabUuWqLBD7+vqSm5treX769Gl0Oh0Au3bt\n4syZMwwbNozy8nLS0tKYO3cuM2bMuOb+zp49d81ll7t8rMx3RzcC0OmU+QrhPLd6mOr4BR/WctHK\n9dhCjWAbddpSjRKKrZ8puAGFn69CF78RZcJEXOa+hNOHSzj/xGjOj3yyUsYXWwt7jX21HUur1vJC\nh/9jTPg4So2lf4detb1lnO6NjK8VtqnKLm3t1KkTmzZtAuDAgQP4+vpahkv07t2bDRs2sGrVKhYv\nXkxYWNh1w/Dtujh++N5UE4pWi8k/oMqOJYQQwgaoVDBkCGf+t5tzkyZDeTkur72KT0RLXF+YjPp4\nak230GZ5Onrh7xJAPad6eDh4ms/+auyseqiDuH1VFogjIiIICwsjJiaGV155hdmzZxMXF8eWLVuq\n6pDX9EvmTlzt3Ig4kIcpqD5obnzohRBCCHEtiqcXJTPncGb/QYpfmY+png6nT5bi3TEC98eHo91d\nPVfICyFuT5WOIX7++ecrPA8NDb1infr16//jHMS3I7vEPH64R/0eOGT/SHmXrlV2LCGEELZJcXXj\n/OhxnB81Gofvv8XpvXdxWL8Oh/Xr0Le/i3Pjn6G81/1yQkaIWqr2zgZeSS7OP9zZ2TzjhbF+cE02\nRwghhDXTail78F/kb44nf+0Gynr2xm53Ih4jH8GrUzscP10G5278mhghRPWw+kB8cfxwl3Lz9G4m\nmXJNCCFEVVOp0N/TmcLPV3FmZxLnh41AczIdt6nP4RMZhvPrc1Hl5PzzfoQQ1cLqA/HF8cNtc813\nwzNKIBZCCFGNjM1DKV64mLy9Byh57nkwGnFZMB+fyDDcxjyOwzerURXk13QzhbBpVh2Is0uySM0/\nxt0BHbE/mQGAqUHDGm6VEEIIW6T4+XFu+izy9h2iaN4bmPwDcPxmDe5jRuHTogke/xqA00dLUJ/4\nq6abKoTNqfu3FrmOi+OHOwVFofk+BZAzxEII2zF37lxSUlJQqVTMmDGD8PBwy7L77rsPf39/NBcu\n8lqwYAF+fn411VTb4uJC6RNjKB01Gs2B33HYtAH7TRuw3xmP/c54XP/vBQwtwijrfT/lvfpgaBMB\naqs+fyVEjbPqQHxx/HCnoM5o0r9D0WhkDmIhhE1ISkrixIkTxMbGkpqayowZM4iNja2wzkcffYSL\ni0sNtVCgUmG8sxXn7mzFuSkvoM7KxH7zxgvheAcuCxfgsnABRl8/ynvdb/7q0g2cnGq65UJYHasO\nxL9k7sTN3p0764WjPplunoNYa9UlCyEEAAkJCfTo0QOAkJAQCgoKKC4uttwgSdQ+poBASh8bRelj\no6C4GPsd281nj7dsxGnFpzit+BTFyYnyrveZw/G93TEFBtV0s4WwClabDvVGPan5x+jV6H60eiPq\n7Cz093Su6WYJIUS1yM3NJSwszPLc29ubnJycCoF49uzZZGRkEBkZyZQpU+ROXrWJqyvlfftT3rc/\nGI1o9+y2DK1w2Lgeh43rATA0D6W8W3fK7+2OvmMnOXssxC2y2kBsp7Hjq35x3OHVHHXGSVSKIlOu\nCSFslqIoFZ5PmjSJLl264OHhwfjx49m0aRO9e/e+7j68vJzRam/sxhI6ndstt7UuqbY6+0Wbv1gI\nR4/Chg2waRPa+Hi0H7yH8wfvgaMjREVBr17mr5YtzbeYvk228F5KjdbjVuu02kAMcF8D88eFmuTt\ngNyUQ4jaaNGihfzxxyHOnMmjtLSUwMAg3N09mDv3jetut2HDd7i4uNK1671XXf7OO28yeHAMgTb6\nkbKvry+5ubmW56dPn0an01meP/jgg5bHUVFRHDly5B8D8dmzN3ZDCZ3OjZycoptscd1TY3V6+sMj\no8xfpaXYJSZgv30r9tu3ot28GTZvhilTMAYEms8c39ud8qhuKF7eN30oW3gvpUbrcbHOWwnFVh2I\nL9KkpwFglCnXhKh1Jk58DjAH3OPHU5kw4dkb2q5Pn/7XXf7MM1Nuu211WadOnVi0aBExMTEcOHAA\nX19fy3CJoqIinn32WZYsWYK9vT27d++mV69eNdxicUscHdF3vRd913spmfMK6uws7OK3Yb/9R+x3\nbMfpixU4fbECRa3G0DaC8q73YmjXAX3rCJRL/kASwtbZRCBWnzQHYhkyIUTdkJy8h6+++pxz584x\nYcJz7Nu3l/j4rZhMJjp27MSoUaNZtuwDPD09adw4hLi4VahUak6c+JNu3bozatRoJkwYzeTJU9m+\nfSslJcWkpZ0gI+MkkyZNoWPHTnz++af8+ONmAgODMBgMxMQMIyKiXU2XXmkiIiIICwsjJiYGlUrF\n7NmziYuLw83NjejoaKKiohg6dCgODg60bNnyH88Oi7rB5B9AWcwwymKGmcce/7of++1bsYvfht3u\nROz27rGsawyqj6FNBPo2bTG0icDQug2Kp1cNtl6ImmMTgViTduEMsQRiIa7JZc5MHL5bW6n7LOv/\nICVzXrmlbVNTj/Hll3HY29uzb99e/vvfpajVaoYMeYChQx+psO7Bgwf44os1mEwmBg/uz6hRoyss\nP336FAsWvMuuXb/w7bdrCAu7k7i4r/nyyzWUlJQQEzOImJhht1xnbfX8889XeB4aGmp5/Nhjj/HY\nY49Vd5NEddJoMLSNxNA2EiZPRVVUiN2uX9Du34d2fzJ2+5JxWL8Oh/XrLJsYGzVG3zYCQ+sIDG0j\n4N5ONViAENXHNgJxepp5DmIbHUsoRF3UtGkz7O3tAXB0dGTChNFoNBry8/MpLCyssG7z5qE4Ojpe\nc1/h4W0A87ja4uJiTp5Mp0mTEBwcHHFwcKRFi7BrbiuEtVDc3CmP7k159IVPAxQFdVYm2n3JaFP2\nYbc/Ge3+ZBy/WQPfrDGvo1Lh1ewO85nkthHmgB3WChwcaq4QIaqATQRidXoapoBAmYNYiOsomfPK\nLZ/NrQp2dnYAZGdnERu7ko8/XomzszOPPjrkinUv3m3tWi5drigKigLqS+78JbONCZukUmEKDKI8\nMMg8vRuYQ/KJv7BL2Yd2XzLOB39FvWcvjkf+wHHVl+ZV7Oww3NkKQ9tI9BfOQBubNpO76Yk6zfoT\nYnm5eQ7iuzrWdEuEELcgPz8fLy8vnJ2d+eOPw2RnZ6PX629rnwEBARw/norBYKCoqIjDhw9VUmuF\nqONUKkyNGlPWqDFlDwzCWedG3qkCNKnH0O7bi92+vWj37UX7+2/Y7UvGiY8AMLm5Y2jT9u+QHBFp\nPhElRB1h9YFYnZmBymSSC+qEqKOaNbsDJydnxo4dRatWbXjggUG8+eZrhIe3vuV9env7EB3dm6ee\nGkHDho1p2TLsH88yC2Gz1GqMze7A2OwOyoY8bH6trAztwd/RJv8dku137sB+5w7LZkb/APMQi/DW\nGFqEYQhtgalRYzmTLGolqw/ElinXJBALUatdOo1aREQ7y4wPGo2Gt95afN1tL50dYv36rQAsXvwh\nAE2aNLUsa9KkqeX14OAGjBplHpc8YkQMAXI2S4gb5+BguWCv9MJLqsIC8wV7+/Zil2wOyQ4/fI/D\nD99bNlOcnDA0D8UY2tISko0tWmLy85exS6JG2U4gljmIhRCXyMvLY/Tox7Czs6dnz974+vrVdJOE\nqNMUdw/0Ud3QR3Xj/IXX1FmZaA/8hubgQbSHD6I9dBDtwQPY7d9XYVuTlxeG0JYYQ1tcCMotMYaG\nyjRwotpYfSBWXwjEJrlLnRDiEo8+OpJHHx1Z080QwqqZAgIpDwiEHpfc+MVgQPPncTSHDpgD8qGD\naA4fxG7XL9gn/K/i9j4+GBuHYGxy5Zfiahu3IhbVw+oDsQyZEEIIIWoRrdYyJrl8wMC/Xz93Du3R\nP9AcOoj28CE0fxxCczzVPARjT9IVuzHpfDE2CcFwaVBuHIKxcRNwcanGgoQ1sPpArE5PQ1GrZQ5i\nIYQQojZzdsbQui2G1m0pu/R1vR5N+gk0x1PNZ5aPp1q+tLsTsUtMuGJXJh8fjEHBmAKDMAUFmR9f\n8q/Jz1+mYhUVWP1Pg+biHMQXJvgXQgghRB1iZ4exSVOMl1wga1FejibtBJrjx/4Oyn8eR51xEu3R\nP1D9uv+qu1Q0Gkz+AeaTZSGNcfHxw1i/PqbgBuZjNWgoucHGVGkgnjt3LikpKahUKmbMmEF4eLhl\n2apVq1i9ejVqtZrQ0FBmz56NqrKvMNXrUWdmYGh/V+XuVwghhBA1z94eY9Nm5huDXE5RUJ09gybj\nJOqMDNQZJ82PM0+iOXkSdWYG2uQ9sDsR58s3Vasx1W+AsXHjC0Mxmvw9JKNhIwnLVqjKAnFSUhIn\nTpwgNjaW1NRUZsyYQWxsLADnz59n/fr1rFy5Ejs7O0aMGMG+ffuIiIio1DaoszJRmUwyfliIWmrM\nmMd57rmphIa2lSCkXQAAEtdJREFUsLz2/vuL8fDw5OGHh1dYNzl5D3Fxq3jlldeZNm0y8+e/VWH5\nmjWx5Ofn88QTY656rGPHjmJvb0+DBg2ZPXs6M2bMxsHh2rd7FkLUcSoVircPBm8faHWNecuNRnSG\nYs7+etgclk/8VWFohv2O7bBje4VNzGE52BySLwblho0x6XSYdL6YdL7gfHnEFrVdlQXihIQEevTo\nAUBISAgFBQUUFxfj6uqKk5MTn332GWAOx8XFxeh0ukpvw99TrkkgFqI2io7uxbZtWyoE4vj4bSxa\n9P51t7s8DN+IHTu2ERrakgYNGvLii/NuenshhBXSaMC/PgYHj6t/mlxcbA7Hfx2vOIb5z+NXDcsX\nmVxcUS4JyOavK58rvr4oLq4yB3MtUGWBODc3l7CwMMtzb29vcnJycHV1tbz24Ycfsnz5ckaMGEFw\ncOVPi2aZci1Y5iAWojbq3r0nY8c+wbhxkwA4fPgQOp2Ov/76k5kzX8DOzg43Nzdeeml+he369u3O\n+vVb2bMniXfffRNvbx98fOoRGBiEwWDg1VfnkJNzmvPnzzNq1Gj8/QP49ts4duzYhpeXF7NmTWf5\n8liKi4uYN+8l9Ho9arWaadP+g0ql4tVX5xAYGMSxY0e5447mTJv2n5r49gghapqrK8ZW4RhbhV+5\nrLgYzV9/ovkzFU1aGuqc05d85aDKOY02eQ8qo/G6h1AcHDB5+6B4+2Dy9sHk433J40te9/ZB8TH/\ni6N8ulXZqu2iOkVRrnht9OjRjBgxgqeeeorIyEgiIyOvub2XlzNa7Y3fWlWnc4O8bADcWoXiprO+\n+Qp1VljT5WyhRqgddf5787/5+uDXlbrPwS0H80bPN4Cr16jTudGoUUOysv4kPDycTz/dwaBBD6JS\n6XnnnYUEBwczdepUDh/ej6enCw4Oduh0bqhUKnQ6N5YtW8LChW8RGhrKU089hYuLA/b2Jrp378bA\ngQNJT0/nmWeeIS4ujq5do+jVqxddu3ZEo1FTr54r77//No88EkOfPn3YuHEjX3zxCRMnTuTIkcMs\nXvwuPj4+REVF4eCg4O7u/o/11ob3UQhRTVxdMd7ZCuOdra69jsmE6syZimE5N8cSmNU5p1GfyUOd\nl4c67QTaA7/d0KEVZxfzGWf/AIz+AZj8/DD5BWDy9zdfLOjnj8nfH8XNXc4+36AqC8S+vr7k5uZa\nnp8+fdoyLCI/P5+jR4/Svn17HB0diYqKIjk5+bqB+OzZczd8bJ3OjZycItz+OIYjkOfqgymn6JZr\nqY0u1mjNbKFGqD11njtfjsl05R+ut7vPnJyi69bYtWsPVq9eS0BAY7Zs+ZElSz7m2LEjvPDCdIxG\nI5mZGYSFtcHfP4CyMj05OUUoikJOThHp6Sfx8QkiJ6eIli3DKSkpo7xcTVLSXlau/AKVSk1e3hly\ncoooLdVTUHCenJwijEYTubnFpKT8ysiRY8jJKaJp0zDefXcRZ86UEBhYH3AkL68Eb+96nDiRjb//\n9f+ncrFGCcVCCAu1GqVePYz16mFs0fKf1y8vR332DKq8PNRn8lBdDMuXP87LQ51z2jztnMl0zd0p\nzs4Y/fwhuD5u3vXModnXD1O9euYzzz5/n3lW3D1sOjxXWSDu1KkTixYtIiYmhgMHDuDr62sZLmEw\nGJg2bRrr1q3DxcWF3377jQEDBlR6G9TpaSgqFaag+pW+byGszZx7XmHOPa9U+3G7dr2X5cs/Jjq6\nF8HBDXB3d2fevJd54423adSoMW+99do1t1Wr1ZbHFz+F2rJlI4WFhbz33lIKCwt58slHr3N0lWU7\nvd6ASmXen0ZT8dOoq33CJYQQlc7e3jxHsp8/1x9ocYHRaD7jnJ2F+lQ26uzsSx5noT51CnV2Fuzc\nieM/9GOKVnshHNerEJQrDN3w9ERx90Dx9MLk7oHi4QEODpVSek2rskAcERFBWFgYMTExqFQqZs+e\nTVxcHG5ubkRHRzN+/HhGjBiBVqulefPmdO/evdLboElPw+QfYDVvlhDWyNnZhZCQZixf/gnR0b0B\nKCkpxs/Pn6KiIpKT9xIScpUplYB69XSkpf1FcHBD9u3bS1hYK/Lz8wkICEStVrNjxzb0ej0AKpUK\n42Vj+Vq0aEly8h6io3uzf//eChf3CSFErafRmIdH+PlfdzWdlxN5B1PNQfnUqcvONueah23k5ppf\nz8xAe+jADTdBcXKyhGPFwxOTh4c5NHt4XAjQniju7ubX3dxR3N1R3D3M27i7m8dD14Iz01U6hvj5\n55+v8Dw0NNTyeNCgQQwaNKjqDm4woM44iSGyfdUdQwhRKaKje/PKK7OZPftlAAYNGszYsU8QHNyA\nYcNG8PHHHzJ69Lgrths9ehwzZ76Av38Avr5+AHTrdh/Tpk3m4MHf6dt3AL6+vnzyyUe0bt2Wt99+\nA+dLpkN68smnmTfvZb77bi1arR3Tp/8Hg8FQPUULIUR10WoxBQSab1R2I/R689jnM3mo83ItAVpV\nWIA6Px9VYQGqgouP882P83JRpR77x4sIL6fY2ZkDs/uFIO3ujuJmDtD6qG6U/WvILRR881RKHfks\n8GbGWOp0buSczMWnRRNKHx5GyauvV2HLakZtGXdalWyhRrCNOm2pRmseQ3yj76EtvN9gG3VKjdah\n2mpUFCgpQV1YgCo/3/KvqrAAVVEh6sJCVAXmx6rCQvPywkLz8sJC1EWFqM79fc2YoUVLzu7YdcOH\nv51+2Hpv3ezgwJl9B1Bcrfd/TkIIIYQQtYZKBa6umFxdITDoxsZBX06vt4Rkk863slt4TdYbiAHF\nw7OmmyCEEEIIIW6UnZ151gsfn2o9rPqfVxFCCCGEEMJ6SSAWQgghhBA2TQKxEEIIIYSwaRKIhRBC\nCCGETZNALIQQQgghbJoEYiGEEEIIYdMkEAshhBBCCJsmgVgIIYQQQti0OnPrZiGEEEIIIaqCnCEW\nQgghhBA2TQKxEEIIIYSwaRKIhRBCCCGETZNALIQQQgghbJoEYiGEEEIIYdMkEAshhBBCCJumrekG\nVKa5c+eSkpKCSqVixowZhIeH13STKs3rr7/O3r17MRgMjBkzhlatWjF16lSMRiM6nY433ngDe3v7\nmm7mbSstLaVfv36MGzeOjh07Wl2N69atY+nSpWi1WiZNmkTz5s2trsaSkhJeeOEFCgoK0Ov1jB8/\nHp1Ox5w5cwBo3rw5L774Ys028hYdOXKEcePGMXLkSIYPH05WVtZV379169bx2WefoVarGTJkCIMH\nD67pplcb6Yfr9u8vSD9c12u05j4YqrAfVqxEYmKiMnr0aEVRFOXYsWPKkCFDarhFlSchIUF58skn\nFUVRlDNnzihdu3ZVpk2bpmzYsEFRFEV58803lZUrV9ZkEyvNW2+9pQwaNEhZs2aN1dV45swZpWfP\nnkpRUZFy6tQpZebMmVZXo6IoyooVK5QFCxYoiqIo2dnZSq9evZThw4crKSkpiqIoyuTJk5X4+Pia\nbOItKSkpUYYPH67MnDlTWbFihaIoylXfv5KSEqVnz55KYWGhcv78eaVv377K2bNna7Lp1Ub64br/\n+6so0g/X9RqttQ9WlKrth61myERCQgI9evQAICQkhIKCAoqLi2u4VZWjffv2vPPOOwC4u7tz/vx5\nEhMT6d69OwD33nsvCQkJNdnESpGamsqxY8fo1q0bgNXVmJCQQMeOHXF1dcXX15eXX37Z6moE8PLy\nIj8/H4DCwkI8PT3JyMiwnCmsq3Xa29vz0Ucf4evra3ntau9fSkoKrVq1ws3NDUdHRyIiIkhOTq6p\nZlcr6Yfr3s/15aQfrvs1WmsfDFXbD1tNIM7NzcXLy8vy3Nvbm5ycnBpsUeXRaDQ4OzsDsHr1aqKi\nojh//rzlIx0fHx+rqPW1115j2rRplufWVuPJkycpLS3l6aef5pFHHiEhIcHqagTo27cvmZmZREdH\nM3z4cKZOnYq7u7tleV2tU6vV4ujoWOG1q71/ubm5eHt7W9axpr7on0g/XPdrlX647tdorX0wVG0/\nbFVjiC+lWOEdqX/88UdWr17Nxx9/TM+ePS2vW0Ota9eupU2bNgQHB191uTXUCJCfn8/ixYvJzMxk\nxIgRFeqylhq//fZbAgMDWbZsGYcPH2b8+PG4ublZlltLnZe7Vl3WWu+NsMbapR+u+6y9H7bVPhhu\nrx+2mkDs6+tLbm6u5fnp06fR6XQ12KLKtXPnTt5//32WLl2Km5sbzs7OlJaW4ujoyKlTpyp8fFAX\nxcfHk56eTnx8PNnZ2djb21tdjT4+PrRt2xatVkuDBg1wcXFBo9FYVY0AycnJdO7cGYDQ0FDKysow\nGAyW5dZSJ3DVn9Gr9UVt2rSpwVZWH+mH6/bPtfTD1lGjLfXBUHn9sNUMmejUqRObNm0C4MCBA/j6\n+uLq6lrDraocRUVFvP7663zwwQd4enoCcM8991jq3bx5M126dKnJJt62t99+mzVr1rBq1SoGDx7M\nuHHjrK7Gzp07s2vXLkwmE2fPnuXcuXNWVyNAw4YNSUlJASAjIwMXFxdCQkLYs2cPYD11wtV/D1u3\nbs1vv/1GYWEhJSUlJCcn065duxpuafWQfrhu/1xLP2wdNdpSHwyV1w+rFCs6d75gwQL27NmDSqVi\n9uzZhIaG1nSTKkVsbCyLFi2icePGltfmz5/PzJkzKSsrIzAwkHnz5mFnZ1eDraw8ixYtIigoiM6d\nO/PCCy9YVY1fffUVq1evBmDs2LG0atXK6mosKSlhxowZ5OXlYTAYeOaZZ9DpdMyaNQuTyUTr1q2Z\nPn16TTfzpv3++++89tprZGRkoNVq8fPzY8GCBUybNu2K92/jxo0sW7YMlUrF8OHDGTBgQE03v9pI\nP1y3f38vkn647tZorX0wVG0/bFWBWAghhBBCiJtlNUMmhBBCCCGEuBUSiIUQQgghhE2TQCyEEEII\nIWyaBGIhhBBCCGHTJBALIYQQQgibJoFY1FqzZs0CzJPFb9++vcKynJwcJk2aBJgnGa+s+7KfP3+e\nzZs3A/DTTz+xZMmSStmvEELUNdIHC1sigVjUSkVFRZZ7r//666+Eh4dXWK7T6Xj33XcBSExMZNeu\nXZVy3IMHD1o646ioKMaOHVsp+xVCiLpE+mBha2QeYlHrxMbGsn37dsrKyggODmbv3r1ERkYyY8YM\nHB0dATh58iSPPPIIK1eu5LHHHkNRFEaMGMGwYcN46aWXOHHiBCUlJfTr149Ro0YRFxdHfHw8BQUF\nPP744wQHBzN79mw0Gg3FxcU8++yztG/fngcffJDCwkIefPBBmjZtyi+//MKCBQtISUlh/vz5aLVa\nVCoVs2bNomnTpjz66KN07NiRffv28ddffzFx4kQGDBjAhg0bWLZsGc7OziiKwrx58wgODq7h76wQ\nQvwz6YOFTVKEqIWWLl2qpKamKoqiKP/5z3+uWJ6enq506dJFURRFeffdd5W33npLURRF+eijj5R3\n3nlHURRFMRgMyqBBg5RDhw4pa9asUXr06KGUlZUpiqIou3btUpKSkhRFUZTk5GRl4MCBiqIoypo1\na5QpU6Zc8bhnz55KSkqKoiiKsm3bNmX48OGKoijK8OHDlTfeeENRFEVJTExU+vfvryiKovTv31/Z\nv3+/oiiKsn//fmX37t2V9r0RQoiqJn2wsDXamg7kQlxNeno6jRo1Ijc3F51Od8PbJSYmkp2dze7d\nuwEoLy8nLS0NgJYtW2Jvbw+YP+57/fXXWbhwIXq9nvz8/Gvus7CwkLy8PMtHhh06dGDy5MmW5R06\ndAAgMDCQgoICAAYNGsS0adPo2bMnPXv2pHXr1jdRvRBC1Czpg4WtkUAsap0nn3ySw4cPk5qaSkFB\nASaTiZycHF566aV/3Nbe3p7x48fTu3fvCq/HxcVVuDf9yy+/TN++fXnooYc4cuQITz/99DX3qVKp\nKjxXLhtlpNVqr1g2cuRI+vXrx86dO5k1axaDBw8mJibmH9svhBA1TfpgYYvkojpR67z33nv06dOH\nFStW0K9fP95///3rdsQqlQqDwQBAZGQkP/zwAwAmk4l58+Zd9cxDbm4uzZo1A2DDhg2Ul5cDoFar\nLfu6yM3NDZ1OR0pKCgAJCQm0adPmmu0xGo0sWLAANzc3Bg4cyMSJEy3bCiFEbSd9sLBFcoZY1DoH\nDx6kRYsWAGRkZFC/fv3rrt+uXTuee+457OzsGDt2LEePHmXo0KEYjUa6deuGp6fnFduMGjWKqVOn\nUr9+fUaOHMmWLVuYP38+gwcPZsGCBUyfPp327dtb1n/ttdeYP38+Go0GtVrNnDlzrtkejUaDl5cX\nMTExlqu0Z86ceQvfCSGEqH7SBwtbJLNMCCGEEEIImyZDJoQQQgghhE2TQCyEEEIIIWyaBGIhhBBC\nCGHTJBALIYQQQgibJoFYCCGEEELYNAnEQgghhBDCpkkgFkIIIYQQNk0CsRBCCCGEsGn/D8im/4tO\nxVz0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "\n",
    "ax[0].set_ylabel('Accuracy');\n",
    "ax[0].set_xlabel('# iterations');\n",
    "lines = ax[0].plot(range(1, 101, 5), history_100.history['acc'][::5], 'r-',\n",
    "                   range(1, 101, 5), history_100.history['val_acc'][::5], 'g-');\n",
    "ax[0].legend(lines, ('Training', 'Validation'));\n",
    "\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('# iterations')\n",
    "lines = ax[1].plot(range(1, 101, 5), history_100.history['loss'][::5], 'r-',\n",
    "                   range(1, 101, 5), history_100.history['val_loss'][::5], 'g-');\n",
    "ax[1].legend(lines, ('Training', 'Validation'))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7182
    },
    "colab_type": "code",
    "id": "jHxkKiL4-R0d",
    "outputId": "409877af-4c93-4a0a-dd28-16d7c66d1d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "391/390 [==============================] - 47s 121ms/step - loss: 0.2482 - acc: 0.9485 - val_loss: 0.6983 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.85700\n",
      "Epoch 2/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.2446 - acc: 0.9501 - val_loss: 0.6322 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.85700\n",
      "Epoch 3/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2422 - acc: 0.9508 - val_loss: 0.6587 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.85700\n",
      "Epoch 4/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2455 - acc: 0.9480 - val_loss: 0.7009 - val_acc: 0.8414\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85700\n",
      "Epoch 5/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2424 - acc: 0.9494 - val_loss: 0.6318 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.85700\n",
      "Epoch 6/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2384 - acc: 0.9518 - val_loss: 0.6729 - val_acc: 0.8480\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.85700\n",
      "Epoch 7/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2394 - acc: 0.9511 - val_loss: 0.6890 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.85700\n",
      "Epoch 8/100\n",
      "391/390 [==============================] - 46s 117ms/step - loss: 0.2374 - acc: 0.9528 - val_loss: 0.6482 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.85700\n",
      "Epoch 9/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.2341 - acc: 0.9539 - val_loss: 0.6451 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85700\n",
      "Epoch 10/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.2340 - acc: 0.9534 - val_loss: 0.6625 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.85700\n",
      "Epoch 11/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2288 - acc: 0.9546 - val_loss: 0.6747 - val_acc: 0.8514\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.85700\n",
      "Epoch 12/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2310 - acc: 0.9540 - val_loss: 0.6546 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85700\n",
      "Epoch 13/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2293 - acc: 0.9546 - val_loss: 0.5879 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.85700 to 0.85980, saving model to gdrive/My Drive/cifar10_model.013.h5\n",
      "Epoch 14/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.2307 - acc: 0.9537 - val_loss: 0.6381 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.85980\n",
      "Epoch 15/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2268 - acc: 0.9543 - val_loss: 0.6143 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85980\n",
      "Epoch 16/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2276 - acc: 0.9541 - val_loss: 0.6438 - val_acc: 0.8522\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.85980\n",
      "Epoch 17/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.2246 - acc: 0.9553 - val_loss: 0.6663 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85980\n",
      "Epoch 18/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2218 - acc: 0.9566 - val_loss: 0.7061 - val_acc: 0.8426\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85980\n",
      "Epoch 19/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2211 - acc: 0.9568 - val_loss: 0.6830 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85980\n",
      "Epoch 20/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2207 - acc: 0.9568 - val_loss: 0.6254 - val_acc: 0.8631\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.85980 to 0.86310, saving model to gdrive/My Drive/cifar10_model.020.h5\n",
      "Epoch 21/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2167 - acc: 0.9575 - val_loss: 0.6518 - val_acc: 0.8548\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.86310\n",
      "Epoch 22/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.2170 - acc: 0.9580 - val_loss: 0.6453 - val_acc: 0.8478\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86310\n",
      "Epoch 23/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2192 - acc: 0.9567 - val_loss: 0.6376 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86310\n",
      "Epoch 24/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2136 - acc: 0.9592 - val_loss: 0.7000 - val_acc: 0.8396\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.86310\n",
      "Epoch 25/100\n",
      "391/390 [==============================] - 46s 117ms/step - loss: 0.2137 - acc: 0.9600 - val_loss: 0.6226 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86310\n",
      "Epoch 26/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2135 - acc: 0.9581 - val_loss: 0.6302 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86310\n",
      "Epoch 27/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2099 - acc: 0.9612 - val_loss: 0.6243 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.86310\n",
      "Epoch 28/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.2097 - acc: 0.9593 - val_loss: 0.6126 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.86310 to 0.86390, saving model to gdrive/My Drive/cifar10_model.028.h5\n",
      "Epoch 29/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.2080 - acc: 0.9610 - val_loss: 0.6493 - val_acc: 0.8526\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.86390\n",
      "Epoch 30/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.2083 - acc: 0.9605 - val_loss: 0.6527 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.86390\n",
      "Epoch 31/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.2059 - acc: 0.9612 - val_loss: 0.6739 - val_acc: 0.8513\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.86390\n",
      "Epoch 32/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2078 - acc: 0.9610 - val_loss: 0.6593 - val_acc: 0.8524\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.86390\n",
      "Epoch 33/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2033 - acc: 0.9626 - val_loss: 0.6193 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.86390\n",
      "Epoch 34/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.2051 - acc: 0.9615 - val_loss: 0.6709 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.86390\n",
      "Epoch 35/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.2026 - acc: 0.9623 - val_loss: 0.7030 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.86390\n",
      "Epoch 36/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.2005 - acc: 0.9630 - val_loss: 0.6319 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.86390\n",
      "Epoch 37/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.1994 - acc: 0.9639 - val_loss: 0.6230 - val_acc: 0.8629\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.86390\n",
      "Epoch 38/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.1982 - acc: 0.9641 - val_loss: 0.7162 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.86390\n",
      "Epoch 39/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.1979 - acc: 0.9638 - val_loss: 0.6771 - val_acc: 0.8506\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.86390\n",
      "Epoch 40/100\n",
      "391/390 [==============================] - 47s 120ms/step - loss: 0.1970 - acc: 0.9637 - val_loss: 0.6663 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.86390\n",
      "Epoch 41/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1945 - acc: 0.9641 - val_loss: 0.6508 - val_acc: 0.8555\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.86390\n",
      "Epoch 42/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1931 - acc: 0.9651 - val_loss: 0.6763 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.86390\n",
      "Epoch 43/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1928 - acc: 0.9655 - val_loss: 0.6800 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.86390\n",
      "Epoch 44/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1927 - acc: 0.9650 - val_loss: 0.6899 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.86390\n",
      "Epoch 45/100\n",
      "391/390 [==============================] - 46s 117ms/step - loss: 0.1952 - acc: 0.9636 - val_loss: 0.7055 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.86390\n",
      "Epoch 46/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1936 - acc: 0.9641 - val_loss: 0.6563 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.86390\n",
      "Epoch 47/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1893 - acc: 0.9658 - val_loss: 0.6107 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00047: val_acc improved from 0.86390 to 0.86460, saving model to gdrive/My Drive/cifar10_model.047.h5\n",
      "Epoch 48/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1891 - acc: 0.9664 - val_loss: 0.6535 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.86460\n",
      "Epoch 49/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1866 - acc: 0.9672 - val_loss: 0.6699 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.86460\n",
      "Epoch 50/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1914 - acc: 0.9650 - val_loss: 0.6594 - val_acc: 0.8553\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.86460\n",
      "Epoch 51/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1889 - acc: 0.9658 - val_loss: 0.6793 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.86460\n",
      "Epoch 52/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1890 - acc: 0.9655 - val_loss: 0.6687 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.86460\n",
      "Epoch 53/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1850 - acc: 0.9671 - val_loss: 0.6224 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.86460\n",
      "Epoch 54/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1837 - acc: 0.9679 - val_loss: 0.6788 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.86460\n",
      "Epoch 55/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1845 - acc: 0.9672 - val_loss: 0.6425 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.86460\n",
      "Epoch 56/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1862 - acc: 0.9665 - val_loss: 0.6516 - val_acc: 0.8606\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.86460\n",
      "Epoch 57/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1796 - acc: 0.9692 - val_loss: 0.6475 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.86460\n",
      "Epoch 58/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1811 - acc: 0.9678 - val_loss: 0.6604 - val_acc: 0.8556\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.86460\n",
      "Epoch 59/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1819 - acc: 0.9678 - val_loss: 0.6595 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.86460\n",
      "Epoch 60/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1784 - acc: 0.9694 - val_loss: 0.7151 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.86460\n",
      "Epoch 61/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1786 - acc: 0.9695 - val_loss: 0.6372 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.86460\n",
      "Epoch 62/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1804 - acc: 0.9684 - val_loss: 0.6898 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.86460\n",
      "Epoch 63/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1799 - acc: 0.9688 - val_loss: 0.6911 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.86460\n",
      "Epoch 64/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1754 - acc: 0.9694 - val_loss: 0.7140 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.86460\n",
      "Epoch 65/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1742 - acc: 0.9711 - val_loss: 0.8173 - val_acc: 0.8344\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.86460\n",
      "Epoch 66/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1768 - acc: 0.9686 - val_loss: 0.6683 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.86460\n",
      "Epoch 67/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1758 - acc: 0.9704 - val_loss: 0.6602 - val_acc: 0.8559\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.86460\n",
      "Epoch 68/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1729 - acc: 0.9709 - val_loss: 0.6616 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.86460\n",
      "Epoch 69/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1737 - acc: 0.9699 - val_loss: 0.7321 - val_acc: 0.8476\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.86460\n",
      "Epoch 70/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1712 - acc: 0.9708 - val_loss: 0.8572 - val_acc: 0.8257\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.86460\n",
      "Epoch 71/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1729 - acc: 0.9708 - val_loss: 0.6377 - val_acc: 0.8619\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.86460\n",
      "Epoch 72/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1711 - acc: 0.9712 - val_loss: 0.7208 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.86460\n",
      "Epoch 73/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1736 - acc: 0.9704 - val_loss: 0.6343 - val_acc: 0.8647\n",
      "\n",
      "Epoch 00073: val_acc improved from 0.86460 to 0.86470, saving model to gdrive/My Drive/cifar10_model.073.h5\n",
      "Epoch 74/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.1703 - acc: 0.9712 - val_loss: 0.6426 - val_acc: 0.8623\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.86470\n",
      "Epoch 75/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1698 - acc: 0.9706 - val_loss: 0.6731 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.86470\n",
      "Epoch 76/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.1684 - acc: 0.9729 - val_loss: 0.6406 - val_acc: 0.8651\n",
      "\n",
      "Epoch 00076: val_acc improved from 0.86470 to 0.86510, saving model to gdrive/My Drive/cifar10_model.076.h5\n",
      "Epoch 77/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1674 - acc: 0.9726 - val_loss: 0.6887 - val_acc: 0.8597\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.86510\n",
      "Epoch 78/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1674 - acc: 0.9725 - val_loss: 0.6992 - val_acc: 0.8548\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.86510\n",
      "Epoch 79/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1654 - acc: 0.9726 - val_loss: 0.6431 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.86510\n",
      "Epoch 80/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.1660 - acc: 0.9721 - val_loss: 0.6590 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.86510\n",
      "Epoch 81/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1657 - acc: 0.9726 - val_loss: 0.6452 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.86510 to 0.86650, saving model to gdrive/My Drive/cifar10_model.081.h5\n",
      "Epoch 82/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1620 - acc: 0.9740 - val_loss: 0.6507 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.86650\n",
      "Epoch 83/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1610 - acc: 0.9738 - val_loss: 0.6443 - val_acc: 0.8624\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.86650\n",
      "Epoch 84/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1622 - acc: 0.9740 - val_loss: 0.6601 - val_acc: 0.8620\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.86650\n",
      "Epoch 85/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1623 - acc: 0.9742 - val_loss: 0.7010 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.86650\n",
      "Epoch 86/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1631 - acc: 0.9736 - val_loss: 0.7057 - val_acc: 0.8538\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.86650\n",
      "Epoch 87/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1631 - acc: 0.9732 - val_loss: 0.6398 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.86650\n",
      "Epoch 88/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1647 - acc: 0.9727 - val_loss: 0.6248 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.86650\n",
      "Epoch 89/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1592 - acc: 0.9746 - val_loss: 0.6520 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.86650\n",
      "Epoch 90/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1597 - acc: 0.9743 - val_loss: 0.6504 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.86650\n",
      "Epoch 91/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.1620 - acc: 0.9729 - val_loss: 0.6633 - val_acc: 0.8593\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.86650\n",
      "Epoch 92/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.1563 - acc: 0.9759 - val_loss: 0.6329 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.86650\n",
      "Epoch 93/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.1566 - acc: 0.9755 - val_loss: 0.6522 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.86650\n",
      "Epoch 94/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1555 - acc: 0.9760 - val_loss: 0.7113 - val_acc: 0.8574\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.86650\n",
      "Epoch 95/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.1567 - acc: 0.9755 - val_loss: 0.6415 - val_acc: 0.8621\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.86650\n",
      "Epoch 96/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1579 - acc: 0.9751 - val_loss: 0.6340 - val_acc: 0.8684\n",
      "\n",
      "Epoch 00096: val_acc improved from 0.86650 to 0.86840, saving model to gdrive/My Drive/cifar10_model.096.h5\n",
      "Epoch 97/100\n",
      "391/390 [==============================] - 47s 119ms/step - loss: 0.1549 - acc: 0.9760 - val_loss: 0.6583 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.86840\n",
      "Epoch 98/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1545 - acc: 0.9757 - val_loss: 0.6673 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.86840\n",
      "Epoch 99/100\n",
      "391/390 [==============================] - 46s 119ms/step - loss: 0.1548 - acc: 0.9753 - val_loss: 0.6451 - val_acc: 0.8626\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.86840\n",
      "Epoch 100/100\n",
      "391/390 [==============================] - 46s 118ms/step - loss: 0.1503 - acc: 0.9770 - val_loss: 0.6404 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.86840\n"
     ]
    }
   ],
   "source": [
    "fit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "colab_type": "code",
    "id": "0tR4Id-IoSIu",
    "outputId": "ca64f617-013a-4ff5-f71d-aec5c2bc8045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('gdrive/My Drive/cifar10_model.096.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7467
    },
    "colab_type": "code",
    "id": "86SNFtHjpB_P",
    "outputId": "285102a7-d8a5-4991-c6bb-2099363ef412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/104\n",
      "391/390 [==============================] - 48s 123ms/step - loss: 0.1565 - acc: 0.9753 - val_loss: 0.6562 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86350, saving model to gdrive/My Drive/cifar10_model.001.h5\n",
      "Epoch 2/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1526 - acc: 0.9763 - val_loss: 0.6869 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.86350\n",
      "Epoch 3/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1541 - acc: 0.9756 - val_loss: 0.6170 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86350 to 0.86500, saving model to gdrive/My Drive/cifar10_model.003.h5\n",
      "Epoch 4/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1533 - acc: 0.9765 - val_loss: 0.7808 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.86500\n",
      "Epoch 5/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1556 - acc: 0.9755 - val_loss: 0.6671 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.86500\n",
      "Epoch 6/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1552 - acc: 0.9764 - val_loss: 0.6293 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.86500 to 0.86880, saving model to gdrive/My Drive/cifar10_model.006.h5\n",
      "Epoch 7/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1522 - acc: 0.9766 - val_loss: 0.7495 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86880\n",
      "Epoch 8/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1524 - acc: 0.9760 - val_loss: 0.7264 - val_acc: 0.8482\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86880\n",
      "Epoch 9/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1515 - acc: 0.9759 - val_loss: 0.6666 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86880\n",
      "Epoch 10/104\n",
      "391/390 [==============================] - 42s 108ms/step - loss: 0.1501 - acc: 0.9767 - val_loss: 0.6452 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86880\n",
      "Epoch 11/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1514 - acc: 0.9766 - val_loss: 0.7048 - val_acc: 0.8556\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.86880\n",
      "Epoch 12/104\n",
      "391/390 [==============================] - 42s 108ms/step - loss: 0.1492 - acc: 0.9774 - val_loss: 0.6522 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.86880\n",
      "Epoch 13/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1471 - acc: 0.9785 - val_loss: 0.6824 - val_acc: 0.8593\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86880\n",
      "Epoch 14/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1501 - acc: 0.9764 - val_loss: 0.6908 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.86880\n",
      "Epoch 15/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1493 - acc: 0.9771 - val_loss: 0.7023 - val_acc: 0.8605\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.86880\n",
      "Epoch 16/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1462 - acc: 0.9781 - val_loss: 0.6659 - val_acc: 0.8638\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86880\n",
      "Epoch 17/104\n",
      "391/390 [==============================] - 42s 109ms/step - loss: 0.1465 - acc: 0.9780 - val_loss: 0.6537 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86880\n",
      "Epoch 18/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1474 - acc: 0.9777 - val_loss: 0.6530 - val_acc: 0.8633\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86880\n",
      "Epoch 19/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1476 - acc: 0.9770 - val_loss: 0.6349 - val_acc: 0.8703\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.86880 to 0.87030, saving model to gdrive/My Drive/cifar10_model.019.h5\n",
      "Epoch 20/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1465 - acc: 0.9771 - val_loss: 0.7550 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87030\n",
      "Epoch 21/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1472 - acc: 0.9768 - val_loss: 0.6448 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87030\n",
      "Epoch 22/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1454 - acc: 0.9777 - val_loss: 0.6706 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87030\n",
      "Epoch 23/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1457 - acc: 0.9778 - val_loss: 0.6743 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87030\n",
      "Epoch 24/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1455 - acc: 0.9782 - val_loss: 0.6857 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87030\n",
      "Epoch 25/104\n",
      "391/390 [==============================] - 42s 109ms/step - loss: 0.1431 - acc: 0.9789 - val_loss: 0.6627 - val_acc: 0.8638\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87030\n",
      "Epoch 26/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1409 - acc: 0.9802 - val_loss: 0.6930 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87030\n",
      "Epoch 27/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1446 - acc: 0.9777 - val_loss: 0.7554 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87030\n",
      "Epoch 28/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1420 - acc: 0.9800 - val_loss: 0.6546 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87030\n",
      "Epoch 29/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1426 - acc: 0.9791 - val_loss: 0.6569 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87030\n",
      "Epoch 30/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1426 - acc: 0.9784 - val_loss: 0.6578 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87030\n",
      "Epoch 31/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1420 - acc: 0.9791 - val_loss: 0.6751 - val_acc: 0.8618\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87030\n",
      "Epoch 32/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1390 - acc: 0.9794 - val_loss: 0.6784 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87030\n",
      "Epoch 33/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1407 - acc: 0.9791 - val_loss: 0.6646 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87030\n",
      "Epoch 34/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1411 - acc: 0.9788 - val_loss: 0.6883 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87030\n",
      "Epoch 35/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1383 - acc: 0.9802 - val_loss: 0.7311 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87030\n",
      "Epoch 36/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1401 - acc: 0.9794 - val_loss: 0.6589 - val_acc: 0.8651\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87030\n",
      "Epoch 37/104\n",
      "391/390 [==============================] - 42s 109ms/step - loss: 0.1387 - acc: 0.9793 - val_loss: 0.6474 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87030\n",
      "Epoch 38/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1385 - acc: 0.9797 - val_loss: 0.7126 - val_acc: 0.8602\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87030\n",
      "Epoch 39/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1391 - acc: 0.9800 - val_loss: 0.6886 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87030\n",
      "Epoch 40/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1369 - acc: 0.9805 - val_loss: 0.6511 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87030\n",
      "Epoch 41/104\n",
      "391/390 [==============================] - 42s 108ms/step - loss: 0.1368 - acc: 0.9805 - val_loss: 0.6896 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87030\n",
      "Epoch 42/104\n",
      "391/390 [==============================] - 42s 109ms/step - loss: 0.1343 - acc: 0.9811 - val_loss: 0.6820 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.87030\n",
      "Epoch 43/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1367 - acc: 0.9808 - val_loss: 0.7201 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.87030\n",
      "Epoch 44/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1374 - acc: 0.9803 - val_loss: 0.6526 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.87030\n",
      "Epoch 45/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1357 - acc: 0.9807 - val_loss: 0.6348 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.87030\n",
      "Epoch 46/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1362 - acc: 0.9808 - val_loss: 0.6795 - val_acc: 0.8603\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.87030\n",
      "Epoch 47/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1349 - acc: 0.9809 - val_loss: 0.6569 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87030\n",
      "Epoch 48/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1348 - acc: 0.9809 - val_loss: 0.6688 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.87030\n",
      "Epoch 49/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1362 - acc: 0.9800 - val_loss: 0.6807 - val_acc: 0.8647\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.87030\n",
      "Epoch 50/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1353 - acc: 0.9803 - val_loss: 0.6927 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87030\n",
      "Epoch 51/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1342 - acc: 0.9803 - val_loss: 0.6375 - val_acc: 0.8699\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.87030\n",
      "Epoch 52/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1323 - acc: 0.9820 - val_loss: 0.6802 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.87030\n",
      "Epoch 53/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1326 - acc: 0.9816 - val_loss: 0.7224 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.87030\n",
      "Epoch 54/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1334 - acc: 0.9813 - val_loss: 0.6530 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.87030\n",
      "Epoch 55/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1322 - acc: 0.9814 - val_loss: 0.6771 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87030\n",
      "Epoch 56/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1315 - acc: 0.9819 - val_loss: 0.6879 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87030\n",
      "Epoch 57/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1315 - acc: 0.9817 - val_loss: 0.6525 - val_acc: 0.8674\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.87030\n",
      "Epoch 58/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1302 - acc: 0.9825 - val_loss: 0.6971 - val_acc: 0.8647\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87030\n",
      "Epoch 59/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1314 - acc: 0.9812 - val_loss: 0.7088 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.87030\n",
      "Epoch 60/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1311 - acc: 0.9819 - val_loss: 0.7136 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.87030\n",
      "Epoch 61/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1295 - acc: 0.9826 - val_loss: 0.6752 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.87030\n",
      "Epoch 62/104\n",
      "391/390 [==============================] - 42s 109ms/step - loss: 0.1301 - acc: 0.9816 - val_loss: 0.6884 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.87030\n",
      "Epoch 63/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1290 - acc: 0.9826 - val_loss: 0.7249 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.87030\n",
      "Epoch 64/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1288 - acc: 0.9824 - val_loss: 0.6579 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.87030\n",
      "Epoch 65/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1289 - acc: 0.9825 - val_loss: 0.6979 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.87030\n",
      "Epoch 66/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1296 - acc: 0.9821 - val_loss: 0.6591 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.87030\n",
      "Epoch 67/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1280 - acc: 0.9824 - val_loss: 0.6573 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00067: val_acc improved from 0.87030 to 0.87130, saving model to gdrive/My Drive/cifar10_model.067.h5\n",
      "Epoch 68/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1285 - acc: 0.9825 - val_loss: 0.6766 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.87130\n",
      "Epoch 69/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1259 - acc: 0.9835 - val_loss: 0.6588 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.87130\n",
      "Epoch 70/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1244 - acc: 0.9837 - val_loss: 0.6638 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.87130\n",
      "Epoch 71/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1261 - acc: 0.9833 - val_loss: 0.6490 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.87130\n",
      "Epoch 72/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1263 - acc: 0.9832 - val_loss: 0.6611 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.87130\n",
      "Epoch 73/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1268 - acc: 0.9831 - val_loss: 0.7060 - val_acc: 0.8633\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.87130\n",
      "Epoch 74/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1236 - acc: 0.9836 - val_loss: 0.6709 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.87130\n",
      "Epoch 75/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1267 - acc: 0.9826 - val_loss: 0.6526 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.87130\n",
      "Epoch 76/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1241 - acc: 0.9834 - val_loss: 0.6633 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.87130\n",
      "Epoch 77/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1253 - acc: 0.9834 - val_loss: 0.6899 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.87130\n",
      "Epoch 78/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1250 - acc: 0.9832 - val_loss: 0.7338 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.87130\n",
      "Epoch 79/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1241 - acc: 0.9832 - val_loss: 0.6888 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.87130\n",
      "Epoch 80/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1253 - acc: 0.9824 - val_loss: 0.6456 - val_acc: 0.8721\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.87130 to 0.87210, saving model to gdrive/My Drive/cifar10_model.080.h5\n",
      "Epoch 81/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1242 - acc: 0.9833 - val_loss: 0.6538 - val_acc: 0.8700\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.87210\n",
      "Epoch 82/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1215 - acc: 0.9843 - val_loss: 0.7135 - val_acc: 0.8588\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.87210\n",
      "Epoch 83/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1229 - acc: 0.9838 - val_loss: 0.6630 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.87210\n",
      "Epoch 84/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1206 - acc: 0.9843 - val_loss: 0.6587 - val_acc: 0.8683\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.87210\n",
      "Epoch 85/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1211 - acc: 0.9842 - val_loss: 0.7023 - val_acc: 0.8603\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.87210\n",
      "Epoch 86/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1205 - acc: 0.9845 - val_loss: 0.6828 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.87210\n",
      "Epoch 87/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1214 - acc: 0.9837 - val_loss: 0.6861 - val_acc: 0.8660\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.87210\n",
      "Epoch 88/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1201 - acc: 0.9849 - val_loss: 0.6608 - val_acc: 0.8689\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.87210\n",
      "Epoch 89/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1208 - acc: 0.9841 - val_loss: 0.7191 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.87210\n",
      "Epoch 90/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1191 - acc: 0.9847 - val_loss: 0.6492 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00090: val_acc improved from 0.87210 to 0.87290, saving model to gdrive/My Drive/cifar10_model.090.h5\n",
      "Epoch 91/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1197 - acc: 0.9854 - val_loss: 0.6805 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.87290\n",
      "Epoch 92/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1174 - acc: 0.9851 - val_loss: 0.6824 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.87290\n",
      "Epoch 93/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1204 - acc: 0.9847 - val_loss: 0.6974 - val_acc: 0.8677\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.87290\n",
      "Epoch 94/104\n",
      "391/390 [==============================] - 42s 109ms/step - loss: 0.1195 - acc: 0.9851 - val_loss: 0.7748 - val_acc: 0.8591\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.87290\n",
      "Epoch 95/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1196 - acc: 0.9848 - val_loss: 0.6772 - val_acc: 0.8691\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.87290\n",
      "Epoch 96/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1197 - acc: 0.9845 - val_loss: 0.6597 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.87290\n",
      "Epoch 97/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1204 - acc: 0.9840 - val_loss: 0.6817 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.87290\n",
      "Epoch 98/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1200 - acc: 0.9845 - val_loss: 0.7185 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.87290\n",
      "Epoch 99/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1172 - acc: 0.9861 - val_loss: 0.6840 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.87290\n",
      "Epoch 100/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1170 - acc: 0.9853 - val_loss: 0.7061 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.87290\n",
      "Epoch 101/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1194 - acc: 0.9841 - val_loss: 0.6730 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.87290\n",
      "Epoch 102/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1165 - acc: 0.9854 - val_loss: 0.6938 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.87290\n",
      "Epoch 103/104\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1170 - acc: 0.9852 - val_loss: 0.6828 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.87290\n",
      "Epoch 104/104\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1161 - acc: 0.9850 - val_loss: 0.7175 - val_acc: 0.8602\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.87290\n"
     ]
    }
   ],
   "source": [
    "history_300 = fit(104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ZrFRCCVqMDR"
   },
   "outputs": [],
   "source": [
    "model.save('gdrive/My Drive/epoch-300-resnet20-v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "XkAQXXWK6-PP",
    "outputId": "94e13181-553a-4cf0-abcc-c2069cd71863"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAEUCAYAAAAsgyAxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "\n",
    "ax[0].set_ylabel('Accuracy');\n",
    "ax[0].set_xlabel('# iterations');\n",
    "lines = ax[0].plot(range(1, 105, 5), history_300.history['acc'][::5], 'r-',\n",
    "                   range(1, 105, 5), history_300.history['val_acc'][::5], 'g-');\n",
    "ax[0].legend(lines, ('Training', 'Validation'));\n",
    "\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('# iterations')\n",
    "lines = ax[1].plot(range(1, 105, 5), history_300.history['loss'][::5], 'r-',\n",
    "                   range(1, 105, 5), history_300.history['val_loss'][::5], 'g-');\n",
    "ax[1].legend(lines, ('Training', 'Validation'))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6646
    },
    "colab_type": "code",
    "id": "CKkfhe487S50",
    "outputId": "673523db-8156-49f1-bf2c-a71361e3b982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1146 - acc: 0.9857 - val_loss: 0.6723 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.87290\n",
      "Epoch 2/100\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1155 - acc: 0.9855 - val_loss: 0.6977 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.87290\n",
      "Epoch 3/100\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1144 - acc: 0.9858 - val_loss: 0.6622 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.87290\n",
      "Epoch 4/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1141 - acc: 0.9858 - val_loss: 0.6545 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.87290 to 0.87390, saving model to gdrive/My Drive/cifar10_model.004.h5\n",
      "Epoch 5/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1153 - acc: 0.9852 - val_loss: 0.6977 - val_acc: 0.8632\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.87390\n",
      "Epoch 6/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1162 - acc: 0.9853 - val_loss: 0.7303 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.87390\n",
      "Epoch 7/100\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.1174 - acc: 0.9845 - val_loss: 0.7100 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.87390\n",
      "Epoch 8/100\n",
      "391/390 [==============================] - 44s 113ms/step - loss: 0.1167 - acc: 0.9853 - val_loss: 0.6853 - val_acc: 0.8697\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.87390\n",
      "Epoch 9/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1136 - acc: 0.9862 - val_loss: 0.6941 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87390\n",
      "Epoch 10/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1152 - acc: 0.9856 - val_loss: 0.6842 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.87390\n",
      "Epoch 11/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1151 - acc: 0.9856 - val_loss: 0.6810 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.87390\n",
      "Epoch 12/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1176 - acc: 0.9844 - val_loss: 0.7238 - val_acc: 0.8626\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.87390\n",
      "Epoch 13/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1148 - acc: 0.9857 - val_loss: 0.7114 - val_acc: 0.8633\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.87390\n",
      "Epoch 14/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1136 - acc: 0.9861 - val_loss: 0.7242 - val_acc: 0.8602\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.87390\n",
      "Epoch 15/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1129 - acc: 0.9858 - val_loss: 0.6893 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87390\n",
      "Epoch 16/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1132 - acc: 0.9868 - val_loss: 0.6911 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.87390\n",
      "Epoch 17/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1124 - acc: 0.9859 - val_loss: 0.6802 - val_acc: 0.8678\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87390\n",
      "Epoch 18/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1122 - acc: 0.9864 - val_loss: 0.6841 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.87390\n",
      "Epoch 19/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1113 - acc: 0.9869 - val_loss: 0.6969 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.87390\n",
      "Epoch 20/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1149 - acc: 0.9853 - val_loss: 0.7008 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87390\n",
      "Epoch 21/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1109 - acc: 0.9864 - val_loss: 0.6840 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87390\n",
      "Epoch 22/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1111 - acc: 0.9861 - val_loss: 0.6623 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87390\n",
      "Epoch 23/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1114 - acc: 0.9861 - val_loss: 0.7146 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87390\n",
      "Epoch 24/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1115 - acc: 0.9862 - val_loss: 0.6797 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87390\n",
      "Epoch 25/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1107 - acc: 0.9866 - val_loss: 0.6861 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87390\n",
      "Epoch 26/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1131 - acc: 0.9854 - val_loss: 0.6547 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87390\n",
      "Epoch 27/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1090 - acc: 0.9877 - val_loss: 0.6494 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87390\n",
      "Epoch 28/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1088 - acc: 0.9870 - val_loss: 0.6776 - val_acc: 0.8666\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87390\n",
      "Epoch 29/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1076 - acc: 0.9874 - val_loss: 0.6929 - val_acc: 0.8687\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87390\n",
      "Epoch 30/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1117 - acc: 0.9866 - val_loss: 0.6715 - val_acc: 0.8693\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87390\n",
      "Epoch 31/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1094 - acc: 0.9868 - val_loss: 0.7795 - val_acc: 0.8553\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87390\n",
      "Epoch 32/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1088 - acc: 0.9874 - val_loss: 0.6634 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87390\n",
      "Epoch 33/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1092 - acc: 0.9868 - val_loss: 0.6847 - val_acc: 0.8695\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87390\n",
      "Epoch 34/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1103 - acc: 0.9866 - val_loss: 0.6940 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87390\n",
      "Epoch 35/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1089 - acc: 0.9871 - val_loss: 0.7484 - val_acc: 0.8578\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87390\n",
      "Epoch 36/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1088 - acc: 0.9875 - val_loss: 0.6932 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87390\n",
      "Epoch 37/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1104 - acc: 0.9859 - val_loss: 0.7370 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87390\n",
      "Epoch 38/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1077 - acc: 0.9872 - val_loss: 0.6861 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87390\n",
      "Epoch 39/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1070 - acc: 0.9874 - val_loss: 0.7035 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87390\n",
      "Epoch 40/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1071 - acc: 0.9877 - val_loss: 0.7374 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87390\n",
      "Epoch 41/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1072 - acc: 0.9876 - val_loss: 0.6907 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87390\n",
      "Epoch 42/100\n",
      "391/390 [==============================] - 44s 111ms/step - loss: 0.1075 - acc: 0.9869 - val_loss: 0.7294 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.87390\n",
      "Epoch 43/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1080 - acc: 0.9876 - val_loss: 0.6866 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.87390\n",
      "Epoch 44/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1070 - acc: 0.9871 - val_loss: 0.7578 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.87390\n",
      "Epoch 45/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1060 - acc: 0.9878 - val_loss: 0.6853 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.87390\n",
      "Epoch 46/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1076 - acc: 0.9876 - val_loss: 0.7154 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.87390\n",
      "Epoch 47/100\n",
      "391/390 [==============================] - 44s 111ms/step - loss: 0.1060 - acc: 0.9873 - val_loss: 0.6766 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87390\n",
      "Epoch 48/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1065 - acc: 0.9874 - val_loss: 0.6777 - val_acc: 0.8723\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.87390\n",
      "Epoch 49/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1063 - acc: 0.9879 - val_loss: 0.7427 - val_acc: 0.8601\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.87390\n",
      "Epoch 50/100\n",
      "391/390 [==============================] - 44s 111ms/step - loss: 0.1082 - acc: 0.9867 - val_loss: 0.6893 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87390\n",
      "Epoch 51/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1051 - acc: 0.9882 - val_loss: 0.7037 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.87390\n",
      "Epoch 52/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1054 - acc: 0.9879 - val_loss: 0.7017 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.87390\n",
      "Epoch 53/100\n",
      "391/390 [==============================] - 44s 111ms/step - loss: 0.1062 - acc: 0.9874 - val_loss: 0.6927 - val_acc: 0.8680\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.87390\n",
      "Epoch 54/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1060 - acc: 0.9871 - val_loss: 0.7167 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.87390\n",
      "Epoch 55/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1033 - acc: 0.9886 - val_loss: 0.6972 - val_acc: 0.8685\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87390\n",
      "Epoch 56/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1036 - acc: 0.9884 - val_loss: 0.7082 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87390\n",
      "Epoch 57/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1031 - acc: 0.9881 - val_loss: 0.6992 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.87390\n",
      "Epoch 58/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1046 - acc: 0.9880 - val_loss: 0.6758 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87390\n",
      "Epoch 59/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1045 - acc: 0.9878 - val_loss: 0.7324 - val_acc: 0.8609\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.87390\n",
      "Epoch 60/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1026 - acc: 0.9883 - val_loss: 0.6995 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.87390\n",
      "Epoch 61/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1064 - acc: 0.9872 - val_loss: 0.7000 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.87390\n",
      "Epoch 62/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1026 - acc: 0.9886 - val_loss: 0.6919 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.87390\n",
      "Epoch 63/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1024 - acc: 0.9887 - val_loss: 0.6807 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.87390\n",
      "Epoch 64/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1047 - acc: 0.9878 - val_loss: 0.7058 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.87390\n",
      "Epoch 65/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1025 - acc: 0.9885 - val_loss: 0.6757 - val_acc: 0.8702\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.87390\n",
      "Epoch 66/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1028 - acc: 0.9885 - val_loss: 0.6903 - val_acc: 0.8685\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.87390\n",
      "Epoch 67/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1016 - acc: 0.9888 - val_loss: 0.7391 - val_acc: 0.8620\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.87390\n",
      "Epoch 68/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1012 - acc: 0.9889 - val_loss: 0.7277 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.87390\n",
      "Epoch 69/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1028 - acc: 0.9879 - val_loss: 0.7009 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.87390\n",
      "Epoch 70/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1013 - acc: 0.9884 - val_loss: 0.7111 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.87390\n",
      "Epoch 71/100\n",
      "391/390 [==============================] - 44s 111ms/step - loss: 0.1015 - acc: 0.9888 - val_loss: 0.7160 - val_acc: 0.8602\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.87390\n",
      "Epoch 72/100\n",
      "391/390 [==============================] - 44s 112ms/step - loss: 0.1021 - acc: 0.9886 - val_loss: 0.7085 - val_acc: 0.8687\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.87390\n",
      "Epoch 73/100\n",
      "391/390 [==============================] - 44s 112ms/step - loss: 0.1013 - acc: 0.9886 - val_loss: 0.6908 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.87390\n",
      "Epoch 74/100\n",
      "391/390 [==============================] - 44s 112ms/step - loss: 0.1034 - acc: 0.9878 - val_loss: 0.7011 - val_acc: 0.8678\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.87390\n",
      "Epoch 75/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1014 - acc: 0.9886 - val_loss: 0.7560 - val_acc: 0.8606\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.87390\n",
      "Epoch 76/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.1011 - acc: 0.9889 - val_loss: 0.7146 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.87390\n",
      "Epoch 77/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1013 - acc: 0.9888 - val_loss: 0.6910 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.87390\n",
      "Epoch 78/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1015 - acc: 0.9884 - val_loss: 0.6839 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.87390\n",
      "Epoch 79/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1024 - acc: 0.9880 - val_loss: 0.6954 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.87390\n",
      "Epoch 80/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.0984 - acc: 0.9896 - val_loss: 0.6819 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.87390\n",
      "Epoch 81/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.0996 - acc: 0.9896 - val_loss: 0.7532 - val_acc: 0.8607\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.87390\n",
      "Epoch 82/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.0984 - acc: 0.9891 - val_loss: 0.7112 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.87390\n",
      "Epoch 83/100\n",
      "391/390 [==============================] - 44s 111ms/step - loss: 0.0977 - acc: 0.9898 - val_loss: 0.7306 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.87390\n",
      "Epoch 84/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1006 - acc: 0.9886 - val_loss: 0.6894 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.87390\n",
      "Epoch 85/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.0997 - acc: 0.9889 - val_loss: 0.7135 - val_acc: 0.8690\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.87390\n",
      "Epoch 86/100\n",
      "391/390 [==============================] - 43s 111ms/step - loss: 0.0982 - acc: 0.9897 - val_loss: 0.6845 - val_acc: 0.8697\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.87390\n",
      "Epoch 87/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.1002 - acc: 0.9885 - val_loss: 0.7100 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.87390\n",
      "Epoch 88/100\n",
      "391/390 [==============================] - 43s 110ms/step - loss: 0.0998 - acc: 0.9885 - val_loss: 0.7072 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.87390\n",
      "Epoch 89/100\n",
      "391/390 [==============================] - 43s 109ms/step - loss: 0.0979 - acc: 0.9900 - val_loss: 0.6789 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.87390\n",
      "Epoch 90/100\n",
      "391/390 [==============================] - 42s 109ms/step - loss: 0.0990 - acc: 0.9892 - val_loss: 0.7269 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.87390\n",
      "Epoch 91/100\n",
      "391/390 [==============================] - 42s 108ms/step - loss: 0.0988 - acc: 0.9894 - val_loss: 0.7175 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.87390\n",
      "Epoch 92/100\n",
      "391/390 [==============================] - 47s 121ms/step - loss: 0.0981 - acc: 0.9893 - val_loss: 0.6826 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.87390\n",
      "Epoch 93/100\n",
      "114/390 [=======>......................] - ETA: 28s - loss: 0.0959 - acc: 0.9903"
     ]
    }
   ],
   "source": [
    "history_400 = fit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72aywwqm7maC"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST, CIFAR-10.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
